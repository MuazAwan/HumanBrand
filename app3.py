# Brand Analysis Agent System using OpenAI Agent SDK with AWS Bedrock
# BEDROCK CLAUDE FIX: Using AgentOutputSchema with strict_json_schema=False
# Running successfully 2 Agents seperate Results successful 8/8/25

import streamlit as st
import json
import os
import asyncio
from datetime import datetime
from typing import Dict, Any, List, Optional, Union, Tuple
import boto3
import tempfile
import markdown
from pathlib import Path
import pandas as pd
from dotenv import load_dotenv
import re
from io import StringIO
import docx
from dataclasses import dataclass
import logging
import time
import math

# Load environment variables
load_dotenv()

# OpenAI Agent SDK imports - Using LiteLLM for Bedrock integration
from agents import Agent, Runner, function_tool, ModelSettings, set_tracing_disabled, AgentOutputSchema
from agents.extensions.models.litellm_model import LitellmModel
from pydantic import BaseModel, Field

# Disable OpenAI tracing since we're not using OpenAI
set_tracing_disabled(True)

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# =================== ENHANCED DOCUMENT PROCESSOR ===================

class DocumentChunker:
    """Professional document chunking strategy based on industry best practices"""
    
    def __init__(self, chunk_size: int = 8000, overlap: int = 800):
        """
        Initialize chunker with optimal parameters from research
        
        Args:
            chunk_size: Target chunk size in characters (8000 chars â‰ˆ 2000 tokens)
            overlap: Overlap between chunks to maintain context (10% of chunk_size)
        """
        self.chunk_size = chunk_size
        self.overlap = overlap
        self.min_chunk_size = 1000  # Minimum viable chunk size
        
    def smart_chunk_by_content(self, content: str, file_name: str) -> List[Dict[str, Any]]:
        """
        Smart chunking strategy that preserves semantic boundaries
        
        Based on research from Pinecone, MongoDB, and other industry sources:
        - Prioritizes paragraph and sentence boundaries
        - Maintains context with overlapping windows
        - Preserves document structure
        """
        
        chunks = []
        
        # Step 1: Try to split by paragraphs first (semantic chunking)
        paragraphs = content.split('\n\n')
        
        current_chunk = ""
        current_chunk_size = 0
        chunk_index = 0
        
        for paragraph in paragraphs:
            paragraph = paragraph.strip()
            if not paragraph:
                continue
                
            paragraph_size = len(paragraph)
            
            # If single paragraph is too large, split it by sentences
            if paragraph_size > self.chunk_size:
                # Split oversized paragraph by sentences
                sentences = re.split(r'(?<=[.!?])\s+', paragraph)
                
                for sentence in sentences:
                    sentence = sentence.strip()
                    if not sentence:
                        continue
                        
                    sentence_size = len(sentence)
                    
                    # If adding this sentence would exceed chunk size, finalize current chunk
                    if current_chunk_size + sentence_size > self.chunk_size and current_chunk:
                        chunks.append(self._create_chunk_metadata(
                            content=current_chunk,
                            chunk_index=chunk_index,
                            file_name=file_name,
                            start_pos=len(''.join([c['content'] for c in chunks])),
                            size=current_chunk_size
                        ))
                        
                        # Start new chunk with overlap
                        overlap_content = self._extract_overlap(current_chunk)
                        current_chunk = overlap_content + " " + sentence
                        current_chunk_size = len(current_chunk)
                        chunk_index += 1
                    else:
                        # Add sentence to current chunk
                        if current_chunk:
                            current_chunk += " " + sentence
                        else:
                            current_chunk = sentence
                        current_chunk_size += sentence_size + (1 if current_chunk else 0)
                        
            else:
                # Normal paragraph processing
                if current_chunk_size + paragraph_size > self.chunk_size and current_chunk:
                    # Finalize current chunk
                    chunks.append(self._create_chunk_metadata(
                        content=current_chunk,
                        chunk_index=chunk_index,
                        file_name=file_name,
                        start_pos=len(''.join([c['content'] for c in chunks])),
                        size=current_chunk_size
                    ))
                    
                    # Start new chunk with overlap
                    overlap_content = self._extract_overlap(current_chunk)
                    current_chunk = overlap_content + "\n\n" + paragraph
                    current_chunk_size = len(current_chunk)
                    chunk_index += 1
                else:
                    # Add paragraph to current chunk
                    if current_chunk:
                        current_chunk += "\n\n" + paragraph
                    else:
                        current_chunk = paragraph
                    current_chunk_size += paragraph_size + (2 if current_chunk else 0)
        
        # Add final chunk if it exists and meets minimum size
        if current_chunk and len(current_chunk) >= self.min_chunk_size:
            chunks.append(self._create_chunk_metadata(
                content=current_chunk,
                chunk_index=chunk_index,
                file_name=file_name,
                start_pos=len(''.join([c['content'] for c in chunks])),
                size=len(current_chunk)
            ))
        
        return chunks
    
    def _extract_overlap(self, text: str) -> str:
        """Extract overlap content from the end of a chunk"""
        if len(text) <= self.overlap:
            return text
        
        # Try to find a good sentence boundary for overlap
        overlap_text = text[-self.overlap:]
        
        # Look for sentence boundary
        sentence_end = overlap_text.rfind('. ')
        if sentence_end > self.overlap // 2:  # If we find a reasonable sentence boundary
            return overlap_text[sentence_end + 2:]
        
        # Otherwise, just use the last overlap characters
        return overlap_text
    
    def _create_chunk_metadata(self, content: str, chunk_index: int, file_name: str, 
                             start_pos: int, size: int) -> Dict[str, Any]:
        """Create metadata for a chunk"""
        return {
            'content': content,
            'chunk_index': chunk_index,
            'file_name': file_name,
            'start_position': start_pos,
            'size': size,
            'word_count': len(content.split()),
            'has_overlap': chunk_index > 0
        }

class EnhancedDocumentProcessor:
    """Enhanced document processor with smart chunking capabilities"""
    
    def __init__(self):
        self.chunker = DocumentChunker()
    
    @staticmethod
    def extract_text_from_docx(file) -> str:
        """Extract text from DOCX file"""
        try:
            doc = docx.Document(file)
            text = []
            for paragraph in doc.paragraphs:
                text.append(paragraph.text)
            return '\n'.join(text)
        except Exception as e:
            logger.error(f"Error extracting text from DOCX: {e}")
            return ""
    
    @staticmethod
    def extract_text_from_md(file) -> str:
        """Extract text from Markdown file"""
        try:
            return file.read().decode('utf-8')
        except Exception as e:
            logger.error(f"Error extracting text from MD: {e}")
            return ""
    
    @staticmethod
    def extract_text_from_txt(file) -> str:
        """Extract text from text file"""
        try:
            return file.read().decode('utf-8')
        except Exception as e:
            logger.error(f"Error extracting text from TXT: {e}")
            return ""
    
    def process_multiple_files_with_chunking(self, uploaded_files) -> Dict[str, Any]:
        """
        FIXED: Process multiple files with intelligent chunking AND content combination
        Returns organized batch data for processing
        """
        
        batch_data = {
            'files': [],
            'total_files': len(uploaded_files),
            'total_chunks': 0,
            'total_words': 0,
            'total_characters': 0,
            'processing_timestamp': datetime.now().isoformat(),
            'chunking_strategy': 'semantic_with_overlap',
            'chunks_by_file': {},
            'combined_content': ""  # FIXED: Add combined content for traditional processing
        }
        
        st.info(f"ðŸ”„ Processing {len(uploaded_files)} files with smart chunking...")
        
        # Create progress tracking
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        # FIXED: Collect all content for combination
        all_content_parts = []
        all_content_parts.append(f"TOTAL_FILES_TO_PROCESS: {len(uploaded_files)}")
        all_content_parts.append(f"PROCESSING_TIMESTAMP: {batch_data['processing_timestamp']}")
        all_content_parts.append("\n" + "="*80 + "\n")
        
        for file_idx, uploaded_file in enumerate(uploaded_files):
            try:
                # FIXED: Reset file pointer to beginning
                uploaded_file.seek(0)
                
                # FIXED: Read content with explicit encoding and error handling
                raw_content = uploaded_file.read()
                
                # FIXED: Handle different file types properly
                if uploaded_file.type == "text/markdown" or uploaded_file.name.endswith('.md'):
                    content = raw_content.decode('utf-8', errors='ignore')
                elif uploaded_file.type == "text/plain" or uploaded_file.name.endswith('.txt'):
                    content = raw_content.decode('utf-8', errors='ignore')
                else:
                    # Fallback for other text types
                    content = raw_content.decode('utf-8', errors='ignore')
                
                # Skip empty files
                if len(content.strip()) < 100:
                    st.warning(f"âš ï¸ File {uploaded_file.name} appears empty or too small")
                    continue
                
                status_text.text(f"Processing file {file_idx + 1}/{len(uploaded_files)}: {uploaded_file.name}")
                
                # FIXED: Add file header for content combination
                file_header = f"""
{'='*60}
FILE_NAME: {uploaded_file.name}
FILE_SIZE: {len(content)} characters
FILE_TYPE: {uploaded_file.type}
{'='*60}
"""
                all_content_parts.append(file_header)
                all_content_parts.append(content)
                
                file_footer = f"""
{'='*60}
END_OF_FILE: {uploaded_file.name}
{'='*60}
"""
                all_content_parts.append(file_footer)
                
                # Smart chunking for batch processing
                chunks = self.chunker.smart_chunk_by_content(content, uploaded_file.name)
                
                # File metadata
                file_data = {
                    'file_name': uploaded_file.name,
                    'file_index': file_idx,
                    'original_size': len(content),
                    'original_word_count': len(content.split()),
                    'chunk_count': len(chunks),
                    'chunks': chunks,
                    'raw_content': content  # FIXED: Store raw content for debugging
                }
                
                batch_data['files'].append(file_data)
                batch_data['total_chunks'] += len(chunks)
                batch_data['total_words'] += file_data['original_word_count']
                batch_data['total_characters'] += file_data['original_size']
                batch_data['chunks_by_file'][uploaded_file.name] = len(chunks)
                
                # Update progress
                progress_bar.progress((file_idx + 1) / len(uploaded_files))
                
                # Display file processing result
                st.success(f"âœ… {uploaded_file.name}: {len(chunks)} chunks, {file_data['original_word_count']:,} words")
                
            except Exception as e:
                st.error(f"âŒ Error processing {uploaded_file.name}: {str(e)}")
                logger.error(f"Error processing file {uploaded_file.name}: {e}")
                continue
        
        # FIXED: Combine all content
        batch_data['combined_content'] = '\n'.join(all_content_parts)
        
        # Final summary
        st.success(f"""
        ðŸ“Š **Processing Complete:**
        - Files processed: {len(batch_data['files'])}
        - Total chunks created: {batch_data['total_chunks']}
        - Total words: {batch_data['total_words']:,}
        - Total characters: {batch_data['total_characters']:,}
        - Combined content length: {len(batch_data['combined_content']):,} characters
        - Average chunks per file: {batch_data['total_chunks'] / len(batch_data['files']):.1f}
        """)
        
        return batch_data

# Define context for dependency injection
@dataclass
class AnalysisContext:
    """Context object for passing data between agents"""
    scraped_content: str = ""
    sitemap_content: str = ""
    
    def __post_init__(self):
        pass

# Replace the entire Pydantic models section in your app3.py with this:

# Final 8/8/25 Define Pydantic models for structured outputs (FIXED for Bedrock Claude)
class AuditMetadata(BaseModel):
    """Audit metadata structure"""
    audit_date: str = Field(..., description="Date of the audit")
    agent_name: str = Field(..., description="Name of the auditing agent")
    persona: str = Field(..., description="Persona description of the agent")

class CorpusBaseline(BaseModel):
    """Corpus baseline metrics"""
    total_pages_analyzed: int = Field(default=0, description="Total number of pages analyzed")
    total_pages_excluded: int = Field(default=0, description="Total number of pages excluded")
    total_words_analyzed: int = Field(default=0, description="Total number of words analyzed")

class NarrativeDataPoints(BaseModel):
    """Narrative data points structure"""
    action_verbs: List[str] = Field(default_factory=list, description="List of action verbs found")
    future_statements: List[str] = Field(default_factory=list, description="List of future-oriented statements")

# âœ… FIXED: Proper structure for lexical frequency data
class LexicalFrequency(BaseModel):
    """Lexical frequency data structure"""
    top_nouns: List[str] = Field(default_factory=list, description="Top nouns found")
    top_verbs: List[str] = Field(default_factory=list, description="Top verbs found")
    proprietary_terms: List[str] = Field(default_factory=list, description="Proprietary terms found")

class VerbalIdentityDataPoints(BaseModel):
    """FIXED: Verbal identity data points structure"""
    linguistic_metrics: Dict[str, str] = Field(default_factory=dict, description="Linguistic analysis metrics")
    lexical_frequency: LexicalFrequency = Field(default_factory=LexicalFrequency, description="Structured lexical frequency analysis")

class AudienceData(BaseModel):
    """Audience data structure"""
    audience_cues: List[str] = Field(default_factory=list, description="Audience targeting cues found")

class EmergentBusinessStructures(BaseModel):
    """Emergent business structures"""
    discovered_product_groups: List[str] = Field(default_factory=list, description="Discovered product groups")
    discovered_strategic_themes: List[str] = Field(default_factory=list, description="Discovered strategic themes")

class NuancedSignals(BaseModel):
    """Nuanced signals in the content"""
    value_laden_words: List[str] = Field(default_factory=list, description="Value-laden words found")
    differentiation_markers: List[str] = Field(default_factory=list, description="Differentiation markers")
    emotional_language_score: Dict[str, str] = Field(default_factory=dict, description="Emotional language scoring")

class ObjectiveDataBedrock(BaseModel):
    """FIXED: Complete structured output for extraction agent (Bedrock Claude compatible)"""
    audit_metadata: AuditMetadata
    corpus_baseline: CorpusBaseline
    narrative_data_points: NarrativeDataPoints
    verbal_identity_data_points: VerbalIdentityDataPoints  # âœ… Now uses fixed structure
    audience_data: AudienceData
    emergent_business_structures: EmergentBusinessStructures
    relationship_matrix: Dict[str, str] = Field(default_factory=dict, description="Relationship matrix")
    nuanced_signals: NuancedSignals

class SynthesisReport(BaseModel):
    """SIMPLIFIED: Structured output for synthesis agent (Bedrock Claude compatible)"""
    report_markdown: str = Field(..., description="Complete markdown report")
    analysis_date: str = Field(..., description="Date of analysis")
    summary: str = Field(default="Brand analysis completed", description="Executive summary")

class BatchProcessingMetadata(BaseModel):
    """Metadata for batch processing"""
    batch_id: str = Field(..., description="Unique batch identifier")
    total_chunks: int = Field(..., description="Total number of chunks processed")
    total_files: int = Field(..., description="Total number of files")
    processing_start: str = Field(..., description="Processing start timestamp")
    processing_end: str = Field(default="", description="Processing end timestamp")
    chunks_processed: int = Field(default=0, description="Number of chunks processed so far")

class ChunkExtractionResult(BaseModel):
    """Result from processing a single chunk"""
    chunk_index: int = Field(..., description="Index of the chunk")
    file_name: str = Field(..., description="Source file name")
    action_verbs: List[str] = Field(default_factory=list, description="Action verbs found in this chunk")
    future_statements: List[str] = Field(default_factory=list, description="Future statements in this chunk")
    product_groups: List[str] = Field(default_factory=list, description="Product groups mentioned")
    strategic_themes: List[str] = Field(default_factory=list, description="Strategic themes identified")
    value_words: List[str] = Field(default_factory=list, description="Value-laden words found")
    audience_cues: List[str] = Field(default_factory=list, description="Audience cues identified")
    word_count: int = Field(default=0, description="Word count of processed chunk")

class AggregatedExtractionResult(BaseModel):
    """Aggregated results from all chunks"""
    batch_metadata: BatchProcessingMetadata
    corpus_baseline: CorpusBaseline
    narrative_data_points: NarrativeDataPoints
    verbal_identity_data_points: VerbalIdentityDataPoints
    audience_data: AudienceData
    emergent_business_structures: EmergentBusinessStructures
    relationship_matrix: Dict[str, str] = Field(default_factory=dict, description="Relationship matrix")
    nuanced_signals: NuancedSignals
    chunk_results: List[ChunkExtractionResult] = Field(default_factory=list, description="Individual chunk results")

class BatchProcessingOrchestrator:
    """PROFESSIONAL: Advanced orchestrator with batch processing capabilities"""
    
    def __init__(self):
        self.doc_processor = EnhancedDocumentProcessor()
        self.max_concurrent_chunks = 3  # Process 3 chunks concurrently to avoid rate limits
        
    async def process_chunk_batch(self, chunk_data: Dict[str, Any], chunk_agent: Agent, 
                                 batch_context: str) -> ChunkExtractionResult:
        """Process a single chunk and return structured results"""
        
        try:
            chunk_prompt = f"""
            CHUNK ANALYSIS TASK:
            
            You are analyzing chunk {chunk_data['chunk_index'] + 1} from file: {chunk_data['file_name']}
            
            CHUNK METADATA:
            - Size: {chunk_data['size']} characters
            - Word count: {chunk_data['word_count']} words
            - Has overlap from previous chunk: {chunk_data['has_overlap']}
            
            BATCH CONTEXT:
            {batch_context}
            
            CHUNK CONTENT:
            {chunk_data['content']}
            
            INSTRUCTIONS:
            Analyze this chunk and extract the following elements. Return ONLY the found elements from THIS chunk:
            
            1. ACTION VERBS: Find "we [verb]" patterns (limit to 10 most relevant)
            2. FUTURE STATEMENTS: Sentences with "will", "future", "tomorrow", "next-gen", "vision" (limit to 5)
            3. PRODUCT GROUPS: Distinct product/service categories mentioned (limit to 5)
            4. STRATEGIC THEMES: Conceptual themes or value propositions (limit to 5)
            5. VALUE WORDS: Value-laden terms like "quality", "innovation", "excellence" (limit to 10)
            6. AUDIENCE CUES: References to "you", "your", "customers", "partners" (limit to 10)
            
            Return results in this exact JSON format:
            {{
                "action_verbs": ["verb1", "verb2"],
                "future_statements": ["statement1", "statement2"],
                "product_groups": ["group1", "group2"],
                "strategic_themes": ["theme1", "theme2"],
                "value_words": ["word1", "word2"],
                "audience_cues": ["cue1", "cue2"]
            }}
            
            Be precise and only include clear, relevant findings from this specific chunk.
            """
            
            # Process chunk with the agent
            result = await Runner.run(chunk_agent, chunk_prompt, max_turns=2)
            
            # Parse the result - expect JSON format
            try:
                if hasattr(result, 'final_output'):
                    json_result = json.loads(str(result.final_output))
                else:
                    json_result = json.loads(str(result))
            except:
                # Fallback parsing if JSON extraction fails
                json_result = {
                    "action_verbs": [],
                    "future_statements": [],
                    "product_groups": [],
                    "strategic_themes": [],
                    "value_words": [],
                    "audience_cues": []
                }
            
            return ChunkExtractionResult(
                chunk_index=chunk_data['chunk_index'],
                file_name=chunk_data['file_name'],
                action_verbs=json_result.get('action_verbs', []),
                future_statements=json_result.get('future_statements', []),
                product_groups=json_result.get('product_groups', []),
                strategic_themes=json_result.get('strategic_themes', []),
                value_words=json_result.get('value_words', []),
                audience_cues=json_result.get('audience_cues', []),
                word_count=chunk_data['word_count']
            )
            
        except Exception as e:
            logger.error(f"Error processing chunk {chunk_data['chunk_index']}: {e}")
            # Return empty result for failed chunk
            return ChunkExtractionResult(
                chunk_index=chunk_data['chunk_index'],
                file_name=chunk_data['file_name'],
                word_count=chunk_data['word_count']
            )
    
    def aggregate_chunk_results(self, chunk_results: List[ChunkExtractionResult], 
                               batch_data: Dict[str, Any]) -> AggregatedExtractionResult:
        """Aggregate results from all processed chunks"""
        
        # Combine all findings
        all_action_verbs = []
        all_future_statements = []
        all_product_groups = []
        all_strategic_themes = []
        all_value_words = []
        all_audience_cues = []
        total_words = 0
        
        for result in chunk_results:
            all_action_verbs.extend(result.action_verbs)
            all_future_statements.extend(result.future_statements)
            all_product_groups.extend(result.product_groups)
            all_strategic_themes.extend(result.strategic_themes)
            all_value_words.extend(result.value_words)
            all_audience_cues.extend(result.audience_cues)
            total_words += result.word_count
        
        # Deduplicate and rank by frequency
        def get_top_items(items: List[str], limit: int = 50) -> List[str]:
            from collections import Counter
            if not items:
                return []
            counter = Counter(items)
            return [item for item, count in counter.most_common(limit)]
        
        # Create aggregated structure
        return AggregatedExtractionResult(
            batch_metadata=BatchProcessingMetadata(
                batch_id=f"batch_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                total_chunks=len(chunk_results),
                total_files=batch_data['total_files'],
                processing_start=batch_data['processing_timestamp'],
                processing_end=datetime.now().isoformat(),
                chunks_processed=len(chunk_results)
            ),
            corpus_baseline=CorpusBaseline(
                total_pages_analyzed=batch_data['total_files'],
                total_words_analyzed=total_words,
                total_pages_excluded=0
            ),
            narrative_data_points=NarrativeDataPoints(
                action_verbs=get_top_items(all_action_verbs),
                future_statements=get_top_items(all_future_statements, 20)
            ),
            verbal_identity_data_points=VerbalIdentityDataPoints(
                linguistic_metrics={
                    "total_chunks_processed": str(len(chunk_results)),
                    "average_chunk_size": str(total_words // len(chunk_results) if chunk_results else 0),
                    "files_processed": str(batch_data['total_files'])
                },
                lexical_frequency=LexicalFrequency(
                    top_nouns=get_top_items(all_product_groups, 20),
                    top_verbs=get_top_items(all_action_verbs, 20),
                    proprietary_terms=get_top_items(all_strategic_themes, 15)
                )
            ),
            audience_data=AudienceData(
                audience_cues=get_top_items(all_audience_cues)
            ),
            emergent_business_structures=EmergentBusinessStructures(
                discovered_product_groups=get_top_items(all_product_groups, 15),
                discovered_strategic_themes=get_top_items(all_strategic_themes, 10)
            ),
            relationship_matrix={
                "product_themes_correlation": "Identified through batch processing",
                "batch_processing_method": "smart_chunking_with_semantic_boundaries"
            },
            nuanced_signals=NuancedSignals(
                value_laden_words=get_top_items(all_value_words),
                differentiation_markers=get_top_items(all_strategic_themes, 15),
                emotional_language_score={"positive_signals": str(len(all_value_words))}
            ),
            chunk_results=chunk_results
        )
    
    async def run_batch_analysis(self, batch_data: Dict[str, Any], 
                               sitemap_content: str) -> Tuple[AggregatedExtractionResult, SynthesisReport, dict]:
        """
        ENHANCED: Run complete batch analysis with chunk-by-chunk processing
        """
        
        agent_results = {
            'batch_extraction_success': False,
            'synthesis_success': False,
            'batch_extraction_error': None,
            'synthesis_error': None,
            'total_chunks_processed': 0,
            'processing_time': 0
        }
        
        start_time = time.time()
        
        try:
            st.info("ðŸ”„ Starting batch processing with smart chunking strategy...")
            
            # Create chunk-specific extraction agent
            chunk_agent = Agent(
                name="Chunk Analyzer",
                instructions="""You are a specialized chunk analyzer. Your job is to extract specific business intelligence from document chunks.
                
                Focus on:
                1. Identifying action verbs in "we [verb]" patterns
                2. Finding future-oriented statements
                3. Detecting product/service categories
                4. Recognizing strategic themes
                5. Spotting value-laden language
                6. Noting audience references
                
                Always return valid JSON with the requested structure. Be precise and relevant.""",
                model=create_bedrock_model(),
                model_settings=ModelSettings(temperature=0.1, max_tokens=2000),
                output_type=str  # Use string output for JSON parsing flexibility
            )
            
            # Prepare batch context
            batch_context = f"""
            BATCH CONTEXT:
            - Total files: {batch_data['total_files']}
            - Total chunks: {batch_data['total_chunks']}
            - Processing approach: Smart semantic chunking
            - Sitemap context: {sitemap_content[:500]}...
            """
            
            # Collect all chunks for processing
            all_chunks = []
            for file_data in batch_data['files']:
                all_chunks.extend(file_data['chunks'])
            
            st.info(f"ðŸ“Š Processing {len(all_chunks)} chunks across {batch_data['total_files']} files...")
            
            # Process chunks in batches to avoid overwhelming the system
            chunk_results = []
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            # Process chunks in small batches
            batch_size = 5
            for i in range(0, len(all_chunks), batch_size):
                batch_chunks = all_chunks[i:i + batch_size]
                
                status_text.text(f"Processing chunk batch {i//batch_size + 1}/{math.ceil(len(all_chunks)/batch_size)}")
                
                # Process batch concurrently but with limited concurrency
                tasks = []
                for chunk_data in batch_chunks:
                    task = self.process_chunk_batch(chunk_data, chunk_agent, batch_context)
                    tasks.append(task)
                
                # Execute batch
                batch_chunk_results = await asyncio.gather(*tasks, return_exceptions=True)
                
                # Filter out exceptions and add successful results
                for result in batch_chunk_results:
                    if isinstance(result, ChunkExtractionResult):
                        chunk_results.append(result)
                    elif isinstance(result, Exception):
                        logger.error(f"Chunk processing error: {result}")
                
                # Update progress
                progress_bar.progress(min((i + batch_size) / len(all_chunks), 1.0))
                
                # Small delay to avoid rate limiting
                await asyncio.sleep(0.5)
            
            agent_results['total_chunks_processed'] = len(chunk_results)
            agent_results['batch_extraction_success'] = True
            
            st.success(f"âœ… Batch extraction completed! Processed {len(chunk_results)} chunks successfully.")
            
            # Aggregate results
            st.info("ðŸ”„ Aggregating results from all chunks...")
            aggregated_results = self.aggregate_chunk_results(chunk_results, batch_data)
            
            # Show immediate results
            with st.expander("ðŸ“Š BATCH PROCESSING RESULTS - View Extracted Data", expanded=False):
                col1, col2 = st.columns(2)
                
                with col1:
                    st.write("**ðŸ“ˆ Processing Statistics:**")
                    st.metric("Chunks Processed", len(chunk_results))
                    st.metric("Total Words Analyzed", aggregated_results.corpus_baseline.total_words_analyzed)
                    st.metric("Action Verbs Found", len(aggregated_results.narrative_data_points.action_verbs))
                    
                    st.write("**ðŸŽ¯ Top Action Verbs:**")
                    for i, verb in enumerate(aggregated_results.narrative_data_points.action_verbs[:10], 1):
                        st.write(f"{i}. {verb}")
                
                with col2:
                    st.write("**ðŸ¢ Business Intelligence:**")
                    st.metric("Product Groups", len(aggregated_results.emergent_business_structures.discovered_product_groups))
                    st.metric("Strategic Themes", len(aggregated_results.emergent_business_structures.discovered_strategic_themes))
                    st.metric("Value Words", len(aggregated_results.nuanced_signals.value_laden_words))
                    
                    st.write("**ðŸŽ¨ Top Value Words:**")
                    for word in aggregated_results.nuanced_signals.value_laden_words[:10]:
                        st.write(f"â€¢ {word}")
            
            # Phase 2: Synthesis with aggregated data
            st.info("ðŸ§¬ Starting synthesis phase with aggregated batch results...")
            
            synthesis_input = f"""
            BATCH-PROCESSED BRAND ANALYSIS DATA:
            
            PROCESSING METADATA:
            - Batch ID: {aggregated_results.batch_metadata.batch_id}
            - Total chunks processed: {aggregated_results.batch_metadata.total_chunks}
            - Total files analyzed: {aggregated_results.batch_metadata.total_files}
            - Total words analyzed: {aggregated_results.corpus_baseline.total_words_analyzed:,}
            
            NARRATIVE FINDINGS:
            - Action verbs identified: {len(aggregated_results.narrative_data_points.action_verbs)}
            - Top action verbs: {', '.join(aggregated_results.narrative_data_points.action_verbs[:20])}
            - Future statements: {len(aggregated_results.narrative_data_points.future_statements)}
            
            BUSINESS STRUCTURE FINDINGS:
            - Product groups discovered: {aggregated_results.emergent_business_structures.discovered_product_groups}
            - Strategic themes identified: {aggregated_results.emergent_business_structures.discovered_strategic_themes}
            
            AUDIENCE & VALUE SIGNALS:
            - Audience cues: {aggregated_results.audience_data.audience_cues[:20]}
            - Value-laden words: {aggregated_results.nuanced_signals.value_laden_words[:20]}
            
            PROCESSING METHOD:
            This analysis used advanced batch processing with semantic chunking to analyze {aggregated_results.batch_metadata.total_chunks} chunks across {aggregated_results.batch_metadata.total_files} files, ensuring comprehensive coverage of all content.
            
            Please synthesize this batch-processed data into a comprehensive brand analysis report.
            """
            
            try:
                synthesis_result = await Runner.run(
                    synthesis_agent,
                    synthesis_input,
                    max_turns=3
                )
                
                synthesis_report = synthesis_result.final_output
                agent_results['synthesis_success'] = True
                
                st.success("âœ… Synthesis phase completed with batch-processed data!")
                
            except Exception as synthesis_error:
                agent_results['synthesis_error'] = str(synthesis_error)
                st.error(f"âŒ Synthesis failed: {synthesis_error}")
                synthesis_report = self._create_fallback_synthesis_report()
            
            agent_results['processing_time'] = time.time() - start_time
            
            return aggregated_results, synthesis_report, agent_results
        except Exception as e:
            agent_results['batch_extraction_error'] = str(e)
            logger.error(f"Batch analysis error: {e}")
            st.error(f"Batch processing error: {str(e)}")
            # Return fallback results
            fallback_results = self._create_fallback_aggregated_results(batch_data)
            fallback_synthesis = self._create_fallback_synthesis_report()
            return fallback_results, fallback_synthesis, agent_results
    
    def _create_fallback_aggregated_results(self, batch_data: Dict[str, Any]) -> AggregatedExtractionResult:
        """Create fallback aggregated results"""
        return AggregatedExtractionResult(
            batch_metadata=BatchProcessingMetadata(
                batch_id=f"fallback_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                total_chunks=batch_data.get('total_chunks', 0),
                total_files=batch_data.get('total_files', 0),
                processing_start=batch_data.get('processing_timestamp', datetime.now().isoformat()),
                processing_end=datetime.now().isoformat(),
                chunks_processed=0
            ),
            corpus_baseline=CorpusBaseline(
                total_pages_analyzed=batch_data.get('total_files', 0),
                total_words_analyzed=batch_data.get('total_words', 0),
                total_pages_excluded=0
            ),
            narrative_data_points=NarrativeDataPoints(
                action_verbs=["analyze", "process", "deliver"],
                future_statements=["We will continue processing documents efficiently"]
            ),
            verbal_identity_data_points=VerbalIdentityDataPoints(
                linguistic_metrics={"fallback_mode": "true"},
                lexical_frequency=LexicalFrequency(
                    top_nouns=["document", "analysis", "content"],
                    top_verbs=["process", "analyze", "extract"],
                    proprietary_terms=["batch_processing"]
                )
            ),
            audience_data=AudienceData(audience_cues=["users", "clients"]),
            emergent_business_structures=EmergentBusinessStructures(
                discovered_product_groups=["document_processing"],
                discovered_strategic_themes=["efficiency", "analysis"]
            ),
            nuanced_signals=NuancedSignals(
                value_laden_words=["efficient", "comprehensive"],
                differentiation_markers=["advanced", "intelligent"],
                emotional_language_score={"neutral": "100%"}
            )
        )
    
    def _create_fallback_synthesis_report(self) -> SynthesisReport:
        """Create fallback synthesis report for batch processing"""
        fallback_markdown = f"""
# The Emergent Brand: Batch-Processed Discovery Report

**Prepared by Humanbrand AI - Batch Processing System**  
**Analysis Date:** {datetime.now().strftime('%Y-%m-%d')}

## Executive Summary

This report was generated using our advanced batch processing system with semantic chunking. The system processes large documents by intelligently breaking them into manageable chunks while preserving context and meaning.

## Batch Processing Methodology

Our system employs industry best practices for large document analysis:

### Smart Chunking Strategy
- **Semantic Boundary Preservation**: Chunks are created at paragraph and sentence boundaries
- **Context Overlap**: 10% overlap between chunks to maintain continuity
- **Optimal Chunk Size**: 8,000 characters (â‰ˆ2,000 tokens) per chunk
- **Parallel Processing**: Multiple chunks processed concurrently

### Technical Architecture
- **Model Provider:** AWS Bedrock (Claude 3.7 Sonnet EU)
- **Processing Framework:** LiteLLM + OpenAI Agents SDK
- **Chunking Engine:** Semantic boundary-aware splitter
- **Aggregation Method:** Frequency-based ranking with deduplication

## System Performance

The batch processing system successfully handled large-scale document analysis that would otherwise exceed context window limitations. This approach ensures comprehensive coverage of all content while maintaining processing efficiency.

## Methodology Benefits

1. **Complete Coverage**: Every document section is analyzed
2. **Context Preservation**: Semantic chunking maintains meaning
3. **Scalability**: Can handle documents of any size
4. **Accuracy**: Parallel processing reduces errors
5. **Efficiency**: Optimized for large-scale analysis

---

*This report demonstrates the capabilities of our enhanced batch processing system for comprehensive brand analysis.*
"""
        
        return SynthesisReport(
            report_markdown=fallback_markdown,
            analysis_date=datetime.now().strftime('%Y-%m-%d'),
            summary="Batch processing system demonstration with semantic chunking capabilities"
        )

# AWS Bedrock client setup
def get_bedrock_client():
    """Initialize AWS Bedrock client"""
    try:
        return boto3.client(
            'bedrock-runtime',
            region_name=os.getenv('AWS_REGION', 'eu-north-1'),
            aws_access_key_id=os.getenv('AWS_ACCESS_KEY_ID'),
            aws_secret_access_key=os.getenv('AWS_SECRET_ACCESS_KEY')
        )
    except Exception as e:
        logger.error(f"Error initializing Bedrock client: {e}")
        raise

def call_bedrock_claude(prompt: str, max_tokens: int = 4000, temperature: float = 0.1) -> str:
    """Call Claude via AWS Bedrock"""
    bedrock_client = get_bedrock_client()
    
    try:
        body = json.dumps({
            "anthropic_version": "bedrock-2023-05-31",
            "max_tokens": max_tokens,
            "temperature": temperature,
            "messages": [
                {
                    "role": "user",
                    "content": prompt
                }
            ]
        })
        
        response = bedrock_client.invoke_model(
            body=body,
            modelId="eu.anthropic.claude-3-7-sonnet-20250219-v1:0",
            accept="application/json",
            contentType="application/json"
        )
        
        response_body = json.loads(response.get('body').read())
        return response_body['content'][0]['text']
        
    except Exception as e:
        logger.error(f"Error calling Bedrock Claude: {e}")
        return f"Error in Claude call: {str(e)}"

# AWS Bedrock client setup
def create_bedrock_model(model_name: str = "eu.anthropic.claude-3-7-sonnet-20250219-v1:0") -> LitellmModel:
    """Create a LiteLLM model instance for AWS Bedrock"""
    
    # Clear any existing AWS profile environment variables that might cause conflicts
    profile_vars_to_clear = ['AWS_PROFILE', 'AWS_DEFAULT_PROFILE']
    for var in profile_vars_to_clear:
        if var in os.environ:
            logger.warning(f"Clearing {var}={os.environ[var]} to avoid profile conflicts")
            del os.environ[var]
    
    # Set AWS credentials explicitly for LiteLLM
    required_env_vars = {
        'AWS_ACCESS_KEY_ID': os.getenv('AWS_ACCESS_KEY_ID', ''),
        'AWS_SECRET_ACCESS_KEY': os.getenv('AWS_SECRET_ACCESS_KEY', ''),
        'AWS_REGION_NAME': os.getenv('AWS_REGION', 'eu-north-1')  # Note: LiteLLM uses AWS_REGION_NAME
    }
    
    # Validate that we have the required credentials
    missing_vars = [k for k, v in required_env_vars.items() if not v]
    if missing_vars:
        raise ValueError(f"Missing required environment variables: {missing_vars}")
    
    # Set the environment variables for LiteLLM
    for var, value in required_env_vars.items():
        os.environ[var] = value
        logger.info(f"Set {var} for LiteLLM")
    
    # Create LiteLLM model with Bedrock
    logger.info(f"Creating Bedrock model with inference profile: {model_name}")
    return LitellmModel(
        model=f"bedrock/{model_name}"
    )

# FIXED: Define agents with AgentOutputSchema(strict_json_schema=False) for Bedrock Claude
extraction_agent = Agent(
    name="The Auditor",
    instructions="""You are the Extraction Agent ("The Auditor") at Humanbrand AI. 
    Your core mandate is ruthless objectivity. 

    ## CRITICAL: MULTI-FILE PROCESSING REQUIREMENTS
    
    ### FILE VALIDATION FIRST
    - Count "FILE_NAME:" occurrences in the input - this is your total_pages_analyzed
    - Look for "TOTAL_FILES_TO_PROCESS:" to verify how many files were processed
    - If you only see 1 file but the summary shows more files expected, FLAG THIS ISSUE
    
    ### COMPREHENSIVE CONTENT ANALYSIS
    You MUST analyze ALL file sections marked with:
    - FILE_NAME: [filename]
    - Content between FILE_NAME and END_OF_FILE markers
    - Each file section contains unique content that must be included in your analysis

    ## CRITICAL GUARDRAILS & EXCLUSIONS

    ### Page-Level Exclusions (Based on Sitemap)
    - Before processing any content from a URL, check if it contains: `/investors/`, `/careers/`, `/legal/`, `/privacy/`, `/terms-of-use/`, `/accessibility/`
    - If a URL matches these patterns, you will NOT analyze its content.

    ### Content-Level Exclusions
    - You are explicitly forbidden from extracting: Meta tags, Alt text, HTML tags, Schema markup, URL structures, Navigation labels
    - Focus exclusively on body text: headlines, subheadings, paragraphs, product descriptions, case studies, news articles, blog posts.

    ## EXTRACTION TASKS:

    Perform the following tasks and return structured data:

    ### Task 1: Corpus Baseline & Filtering
    - total_pages_analyzed = Count of "FILE_NAME:" occurrences
    - total_words_analyzed = Sum of words from ALL file sections
    - total_pages_excluded = Files matching exclusion patterns
    - Minimum 20 instances required for patterns

    ### Task 2: Narrative Data Points
    - Action Verb Patterns: Find "we [verb]" patterns across ALL files, top 50 most frequent
    - Future-Oriented Statements: Count sentences with "will", "future", "tomorrow", "next-gen", "vision" across ALL files

    ### Task 3: Verbal Identity Data Points
    - Linguistic Metrics: Calculate across ALL combined content
    - Lexical Frequency: Extract from ALL files combined
      - top_nouns: List of top 20 nouns across all content
      - top_verbs: List of top 20 verbs across all content  
      - proprietary_terms: List of company-specific terms found

    ### Task 4: Audience Data Points
    - Count addressed language from ALL files: "you", "your", "partners", "customers", etc.

    ### Task 5: Discover Emergent Business Structures
    - Product Groups: Identify ALL distinct product/service categories across files
    - Strategic Themes: Extract conceptual clusters from ALL content

    ### Task 6: Map Relationships
    - Create co-occurrence matrix between discovered elements

    ### Task 7: Nuanced Signal Extraction
    - Value-laden words: Extract from ALL content
    - Differentiation markers: Find across ALL files
    - Emotional language: Analyze combined sentiment

    ## QUALITY VALIDATION
    Before returning results, verify:
    - total_pages_analyzed matches the file count in TOTAL_FILES_TO_PROCESS
    - You found diverse content themes (not just one topic)
    - Word count is substantial (5000+ words expected for multiple files)
    
    If you're only finding one topic across multiple files, re-examine the input for additional content sections.

    Return the analysis in the required structured format with confidence that you've processed ALL provided content.
    
    IMPORTANT: Since you're using Bedrock Claude, ensure all outputs are valid JSON-compatible structures.
    Use simple data types (strings, numbers, arrays) and avoid complex nested objects where possible.""",
    model=create_bedrock_model(),
    model_settings=ModelSettings(
        temperature=0.1,
        max_tokens=4000
    ),
    # FIXED: Using AgentOutputSchema with strict_json_schema=False for Bedrock Claude
    output_type=AgentOutputSchema(ObjectiveDataBedrock, strict_json_schema=False)
)

synthesis_agent = Agent(
    name="The Archaeologist", 
    instructions="""You are the Synthesis Agent ("The Archaeologist") at Humanbrand AI, operating as a Senior AI Researcher.

    ## CORE METHODOLOGY
    - **Amnesia Protocol:** Analyze ONLY the provided objective data
    - **Human-Centric Strategic Synthesis:** Find higher-order ideas from literal patterns
    - **Strictly neutral and descriptive tone** - avoid laudatory language
    - **Two-Tiered Synthesis:** Provide "Synthesized Finding" and "Deep Rationale" with "Strategic Implication"

    Generate a complete Markdown report with this structure:

    # The Emergent Brand: A Discovery Report

    **Prepared by Humanbrand AI**  
    **Analysis Date:** [Current Date]

    ## Executive Summary
    [Synthesize a 1-2 paragraph summary of key findings]

    ## Introduction
    This Humanbrand AI report presents the emergent brand reality discovered through our rigorous outside-in audit.

    ## Corpus Analysis Summary
    * **Total Pages Analyzed:** [from corpus_baseline]
    * **Total Words Analyzed:** [from corpus_baseline]
    * **Total Pages Excluded:** [from corpus_baseline]

    ## Synthesis Overview
    [1-2 paragraph high-level summary of brand nature]

    ---

    ## I. The Brand Narrative & Platform

    ### 1. Mission: "What do we demonstrably do every day to create value?"
    **Synthesized Finding:** [Strategic abstraction of core function]
    **Deep Rationale:** [Analysis citing data with Strategic Implication subsection]

    ### 2. Vision: "What is the future reality we are actively trying to build?"
    **Synthesized Finding:** [Abstraction from future_statements]
    **Deep Rationale:** [Analysis citing data with Strategic Implication subsection]

    Continue with the complete structure as specified in the original prompt...

    Return both the markdown report and a summary in the structured format.
    
    IMPORTANT: Since you're using Bedrock Claude, ensure all outputs are valid JSON-compatible structures.
    Keep string values clean and properly escaped for JSON serialization.""",
    model=create_bedrock_model(),
    model_settings=ModelSettings(
        temperature=0.3,
        max_tokens=8000
    ),
    # FIXED: Using AgentOutputSchema with strict_json_schema=False for Bedrock Claude
    output_type=AgentOutputSchema(SynthesisReport, strict_json_schema=False)
)

class BrandAnalysisOrchestrator:
    """ENHANCED: Main orchestrator with first agent results visibility"""
    
    def __init__(self):
        self.doc_processor = EnhancedDocumentProcessor()
    
    async def run_analysis(self, scraped_content: str, sitemap_content: str) -> tuple[ObjectiveDataBedrock, SynthesisReport, dict]:
        """ENHANCED: Run the complete analysis pipeline with first agent results tracking"""
        
        # Initialize results tracking
        agent_results = {
            'extraction_raw_output': None,
            'extraction_structured_output': None,
            'synthesis_raw_output': None,
            'synthesis_structured_output': None,
            'extraction_success': False,
            'synthesis_success': False,
            'extraction_error': None,
            'synthesis_error': None
        }
        
        try:
            logger.info("Starting brand analysis pipeline with Bedrock Claude")
            st.info("ðŸ” Starting extraction phase with The Auditor (Bedrock Claude)...")
            
            # Phase 1: Data Extraction with detailed tracking
            extraction_input = f"""Extract objective data from the provided corpus content:

MULTI-FILE CONTENT ANALYSIS:
This content contains multiple files that have been combined for analysis. Look for:
- "TOTAL_FILES_TO_PROCESS:" to identify the number of files
- "FILE_NAME:" markers indicating individual file sections
- "END_OF_FILE:" markers showing where each file ends

SCRAPED CONTENT PREVIEW (first 5000 chars to show file structure):
{scraped_content[:5000]}...

SITEMAP CONTENT (first 1000 chars):
{sitemap_content[:1000]}...

FULL CONTENT STATS:
- Scraped content length: {len(scraped_content)} characters
- Sitemap content length: {len(sitemap_content)} characters
- Expected files: Look for "TOTAL_FILES_TO_PROCESS:" in the content

CRITICAL INSTRUCTION: You must analyze ALL files in the content, not just the first one. Count the "FILE_NAME:" occurrences to determine total_pages_analyzed.

Please analyze this content according to your instructions and return structured data.

IMPORTANT: Ensure your response is a valid JSON object that matches the ObjectiveDataBedrock schema. Use simple string and array values to ensure compatibility with Bedrock Claude."""
            
            try:
                # Run extraction agent with detailed tracking
                extraction_result = await Runner.run(
                    extraction_agent,
                    extraction_input,
                    max_turns=3
                )
                
                # Capture raw output for debugging
                agent_results['extraction_raw_output'] = str(extraction_result)
                agent_results['extraction_structured_output'] = extraction_result.final_output
                agent_results['extraction_success'] = True
                
                # Access the structured output
                objective_data = extraction_result.final_output
                logger.info("âœ… Extraction phase completed with Bedrock Claude structured output")
                
                # Display first agent results in Streamlit
                st.success("âœ… Extraction phase completed with Bedrock Claude!")
                
                # ENHANCEMENT: Show first agent results immediately
                with st.expander("ðŸ” THE AUDITOR (First Agent) Results - EXPAND TO VIEW", expanded=False):
                    st.subheader("ðŸ“Š Extraction Agent Output Details")
                    
                    # Show structured data preview
                    col1, col2 = st.columns(2)
                    
                    with col1:
                        st.write("**ðŸ“ˆ Corpus Analysis:**")
                        st.metric("Pages Analyzed", objective_data.corpus_baseline.total_pages_analyzed)
                        st.metric("Words Analyzed", objective_data.corpus_baseline.total_words_analyzed)
                        st.metric("Pages Excluded", objective_data.corpus_baseline.total_pages_excluded)
                        
                        st.write("**ðŸŽ¯ Action Verbs Found:**")
                        if objective_data.narrative_data_points.action_verbs:
                            for i, verb in enumerate(objective_data.narrative_data_points.action_verbs[:10], 1):
                                st.write(f"{i}. {verb}")
                        else:
                            st.write("No action verbs found")
                    
                    with col2:
                        st.write("**ðŸ¢ Business Structures:**")
                        if objective_data.emergent_business_structures.discovered_product_groups:
                            st.write("*Product Groups:*")
                            for group in objective_data.emergent_business_structures.discovered_product_groups:
                                st.write(f"â€¢ {group}")
                        
                        if objective_data.emergent_business_structures.discovered_strategic_themes:
                            st.write("*Strategic Themes:*")
                            for theme in objective_data.emergent_business_structures.discovered_strategic_themes:
                                st.write(f"â€¢ {theme}")
                        
                        st.write("**ðŸŽ¨ Value-laden Words:**")
                        if objective_data.nuanced_signals.value_laden_words:
                            for word in objective_data.nuanced_signals.value_laden_words[:10]:
                                st.write(f"â€¢ {word}")
                    
                    # Show detailed raw output for debugging
                    st.write("**ðŸ”§ Raw Agent Output (First 2000 characters):**")
                    raw_output_preview = str(extraction_result)[:2000]
                    st.code(raw_output_preview + "..." if len(str(extraction_result)) > 2000 else raw_output_preview)
                    
                    # Show JSON structure validation
                    st.write("**âœ… Data Validation:**")
                    validation_col1, validation_col2 = st.columns(2)
                    
                    with validation_col1:
                        st.write(f"â€¢ Audit Date: {objective_data.audit_metadata.audit_date}")
                        st.write(f"â€¢ Agent Name: {objective_data.audit_metadata.agent_name}")
                        st.write(f"â€¢ Total Action Verbs: {len(objective_data.narrative_data_points.action_verbs)}")
                        st.write(f"â€¢ Total Future Statements: {len(objective_data.narrative_data_points.future_statements)}")
                    
                    with validation_col2:
                        st.write(f"â€¢ Product Groups Found: {len(objective_data.emergent_business_structures.discovered_product_groups)}")
                        st.write(f"â€¢ Strategic Themes: {len(objective_data.emergent_business_structures.discovered_strategic_themes)}")
                        st.write(f"â€¢ Value Words: {len(objective_data.nuanced_signals.value_laden_words)}")
                        st.write(f"â€¢ Audience Cues: {len(objective_data.audience_data.audience_cues)}")
            
            except Exception as extraction_error:
                agent_results['extraction_error'] = str(extraction_error)
                agent_results['extraction_success'] = False
                st.error(f"âŒ Extraction agent failed: {extraction_error}")
                # Create fallback data
                objective_data = self._create_fallback_objective_data(scraped_content)
            
            st.info("ðŸ§¬ Starting synthesis phase with The Archaeologist (Bedrock Claude)...")
            
            # Phase 2: Brand Synthesis with detailed tracking
            synthesis_input = f"""Synthesize a brand analysis report from this objective data:

AUDIT METADATA:
- Date: {objective_data.audit_metadata.audit_date}
- Agent: {objective_data.audit_metadata.agent_name}

CORPUS BASELINE:
- Pages Analyzed: {objective_data.corpus_baseline.total_pages_analyzed}
- Words Analyzed: {objective_data.corpus_baseline.total_words_analyzed}
- Pages Excluded: {objective_data.corpus_baseline.total_pages_excluded}

NARRATIVE DATA POINTS:
- Action Verbs: {objective_data.narrative_data_points.action_verbs[:10]}... (showing first 10)
- Future Statements: {objective_data.narrative_data_points.future_statements[:5]}... (showing first 5)

EMERGENT BUSINESS STRUCTURES:
- Product Groups: {objective_data.emergent_business_structures.discovered_product_groups}
- Strategic Themes: {objective_data.emergent_business_structures.discovered_strategic_themes}

NUANCED SIGNALS:
- Value-laden Words: {objective_data.nuanced_signals.value_laden_words}
- Differentiation Markers: {objective_data.nuanced_signals.differentiation_markers}

Please synthesize this data into a comprehensive brand analysis report according to your instructions.

IMPORTANT: Ensure your response is a valid JSON object that matches the SynthesisReport schema. Keep the markdown properly escaped for JSON serialization."""
            
            try:
                synthesis_result = await Runner.run(
                    synthesis_agent,
                    synthesis_input,
                    max_turns=3
                )
                
                # Capture synthesis results
                agent_results['synthesis_raw_output'] = str(synthesis_result)
                agent_results['synthesis_structured_output'] = synthesis_result.final_output
                agent_results['synthesis_success'] = True
                
                # Access the structured output
                synthesis_report = synthesis_result.final_output
                logger.info("âœ… Synthesis phase completed with Bedrock Claude structured output")
                
                st.success("âœ… Synthesis phase completed with Bedrock Claude!")
                
            except Exception as synthesis_error:
                agent_results['synthesis_error'] = str(synthesis_error)
                agent_results['synthesis_success'] = False
                st.error(f"âŒ Synthesis agent failed: {synthesis_error}")
                # Create fallback synthesis report
                synthesis_report = self._create_fallback_synthesis_report()
            
            logger.info("Brand analysis pipeline completed successfully with Bedrock Claude")
            
            # Return enhanced results including agent tracking
            return objective_data, synthesis_report, agent_results
            
        except Exception as e:
            logger.error(f"Analysis pipeline error: {e}")
            st.error(f"Analysis pipeline error: {str(e)}")
            # Return fallback structured objects with error info
            agent_results['extraction_error'] = str(e)
            agent_results['synthesis_error'] = str(e)
            fallback_objective = self._create_fallback_objective_data(scraped_content)
            fallback_synthesis = self._create_fallback_synthesis_report()
            return fallback_objective, fallback_synthesis, agent_results
    
    def _create_fallback_objective_data(self, scraped_content: str) -> ObjectiveDataBedrock:
        """FIXED: Create fallback structured data for Bedrock Claude"""
        return ObjectiveDataBedrock(
            audit_metadata=AuditMetadata(
                audit_date=datetime.now().strftime('%Y-%m-%d'),
                agent_name="The Auditor (Bedrock Claude Fallback)",
                persona="Fallback Data Extractor"
            ),
            corpus_baseline=CorpusBaseline(
                total_pages_analyzed=0,
                total_words_analyzed=len(scraped_content.split()),
                total_pages_excluded=0
            ),
            narrative_data_points=NarrativeDataPoints(
                action_verbs=["analyze", "create", "deliver"],
                future_statements=["We will continue to innovate"]
            ),
            verbal_identity_data_points=VerbalIdentityDataPoints(
                linguistic_metrics={"average_sentence_length": "15 words"},
                lexical_frequency={"top_nouns": "product, service, customer"}
            ),
            audience_data=AudienceData(
                audience_cues=["customers", "partners", "users"]
            ),
            emergent_business_structures=EmergentBusinessStructures(
                discovered_product_groups=["core products", "services"],
                discovered_strategic_themes=["innovation", "quality"]
            ),
            relationship_matrix={"products_themes": "innovation focus"},
            nuanced_signals=NuancedSignals(
                value_laden_words=["quality", "innovation"],
                differentiation_markers=["unique", "leading"],
                emotional_language_score={"positive": "70%", "negative": "30%"}
            )
        )
    
    def _create_fallback_synthesis_report(self) -> SynthesisReport:
        """FIXED: Create fallback synthesis report for Bedrock Claude"""
        fallback_markdown = f"""
# The Emergent Brand: A Discovery Report

**Prepared by Humanbrand AI**  
**Analysis Date:** {datetime.now().strftime('%Y-%m-%d')}

## Executive Summary

This report was generated using fallback processing due to an issue with the main analysis pipeline.

## Technical Details

- **System:** Brand Analysis Agent System with LiteLLM + Bedrock Claude
- **Model Provider:** AWS Bedrock (Claude 3.7 Sonnet EU)
- **Integration:** LiteLLM + OpenAI Agents SDK (Non-Strict JSON Schema)
- **Environment:** {os.getenv('AWS_REGION', 'us-east-1')}

Please check the logs for more information about the analysis process.
"""
        
        return SynthesisReport(
            report_markdown=fallback_markdown,
            analysis_date=datetime.now().strftime('%Y-%m-%d'),
            summary="Fallback report generated due to analysis pipeline issue with Bedrock Claude"
        )
    


def validate_environment() -> bool:
    """Validate required environment variables and AWS key format"""
    required_vars = ['AWS_ACCESS_KEY_ID', 'AWS_SECRET_ACCESS_KEY']
    missing_vars = [var for var in required_vars if not os.getenv(var)]
    
    if missing_vars:
        st.error(f"Missing required environment variables: {', '.join(missing_vars)}")
        st.info("Note: This system uses AWS Bedrock through LiteLLM integration, not OpenAI API.")
        return False
    
    # Validate AWS Access Key ID format
    aws_access_key_id = os.getenv('AWS_ACCESS_KEY_ID', '')
    if aws_access_key_id:
        if not (aws_access_key_id.startswith('AKIA') or aws_access_key_id.startswith('ASIA')):
            st.error("ðŸš¨ **Invalid AWS Access Key ID Format!**")
            st.markdown("""
            **Error Details:**
            Your AWS Access Key ID must start with either:
            - `AKIA` (for long-term IAM user credentials)
            - `ASIA` (for temporary STS credentials)
            
            **Current Key Format:** Your key doesn't start with the required prefix.
            
            **How to Fix:**
            1. **Create new IAM user access keys** (recommended):
               ```bash
               aws iam create-access-key --user-name your-username
               ```
            
            2. **Or use your existing IAM console**:
               - Go to AWS Console â†’ IAM â†’ Users â†’ [Your User] â†’ Security Credentials
               - Click "Create access key"
               - Copy the new Access Key ID (starts with AKIA)
            
            3. **Update your environment variables:**
               ```bash
               export AWS_ACCESS_KEY_ID="AKIA..."  # New key starting with AKIA
               export AWS_SECRET_ACCESS_KEY="..."   # Corresponding secret
               ```
            
            4. **For temporary credentials (ASIA), you also need:**
               ```bash
               export AWS_SESSION_TOKEN="..."
               ```
            """)
            return False
        
        # Validate key length (should be 20 characters)
        if len(aws_access_key_id) != 20:
            st.warning(f"âš ï¸ AWS Access Key ID length is {len(aws_access_key_id)} characters. Standard AWS keys are 20 characters.")
    
    # Check for conflicting AWS profile environment variables
    profile_vars = ['AWS_PROFILE', 'AWS_DEFAULT_PROFILE']
    conflicting_vars = [(var, os.getenv(var)) for var in profile_vars if os.getenv(var)]
    
    if conflicting_vars:
        st.warning(f"âš ï¸ Found AWS profile environment variables that may cause conflicts:")
        for var, value in conflicting_vars:
            st.code(f"{var}={value}")
        st.info("ðŸ’¡ These will be automatically cleared when running the analysis to use direct credentials instead.")
    
    # Test LiteLLM integration
    try:
        import litellm
        logger.info("LiteLLM integration available")
        return True
    except ImportError:
        st.error("LiteLLM not installed. Please run: pip install litellm")
        return False

def main():
    """FIXED: Main Streamlit application with Bedrock Claude compatibility"""
    
    st.set_page_config(
        page_title="Brand Analysis Agent System",
        page_icon="ðŸ§¬",
        layout="wide",
        initial_sidebar_state="expanded"
    )
    
    st.title("ðŸ§¬ Brand Analysis Agent System")
    st.markdown("### AI-Powered Brand Discovery using Bedrock Claude 3.7 (NON-STRICT JSON SCHEMA FIX)")
    
    # Show the fix information
    st.info("""
    ðŸ”§ **BEDROCK CLAUDE FIX APPLIED:**
    - Using `AgentOutputSchema(Model, strict_json_schema=False)` for both agents
    - Simplified Pydantic models to avoid complex nested structures
    - Compatible with LiteLLM + Bedrock Claude limitations
    """)
    
    # Validate environment
    if not validate_environment():
        st.stop()
    
    # Initialize session state for file processing
    if 'files_processed' not in st.session_state:
        st.session_state.files_processed = False
    if 'scraped_content' not in st.session_state:
        st.session_state.scraped_content = ""
    if 'sitemap_content' not in st.session_state:
        st.session_state.sitemap_content = ""
    if 'analysis_results' not in st.session_state:
        st.session_state.analysis_results = None
    
    # Sidebar for file uploads
    with st.sidebar:
        st.header("ðŸ“ Upload Documents")
        
        # Multiple file uploader for .md files
        scraped_corpus_files = st.file_uploader(
            "Upload Multiple Scraped Corpus Files (.md, .txt)",
            type=['md', 'txt'],
            help="Upload multiple scraped website content files (up to 6 files)",
            accept_multiple_files=True
        )
        
        sitemap_file = st.file_uploader(
            "Upload Sitemap (.docx, .txt)",
            type=['docx', 'txt'],
            help="Upload the sitemap document"
        )
        
        # Process files button
        if st.button("ðŸ“‚ Process Uploaded Files", use_container_width=True):
            if scraped_corpus_files:
                try:
                    with st.spinner("Processing files..."):
                        # âœ… FIXED: Create instance first
                        enhanced_processor = EnhancedDocumentProcessor()
                        
                        # ðŸ” DEBUG: Check files before processing
                        st.write("**ðŸ” DEBUG: File Analysis Before Processing**")
                        for i, file in enumerate(scraped_corpus_files):
                            file.seek(0)  # Reset pointer
                            raw_content = file.read()
                            file.seek(0)  # Reset again
                            
                            st.write(f"File {i+1}: {file.name}")
                            st.write(f"  - Raw size: {len(raw_content)} bytes")
                            st.write(f"  - File type: {file.type}")
                            
                            # Try to decode
                            try:
                                decoded_content = raw_content.decode('utf-8', errors='ignore')
                                st.write(f"  - Decoded size: {len(decoded_content)} characters")
                                st.write(f"  - First 100 chars: {decoded_content[:100]}")
                            except Exception as e:
                                st.error(f"  - Decode error: {e}")
                        
                        # Process multiple scraped corpus files with chunking
                        st.info(f"Processing {len(scraped_corpus_files)} corpus files...")
                        batch_data = enhanced_processor.process_multiple_files_with_chunking(scraped_corpus_files)
                        
                        # ðŸ” DEBUG: Check batch data after processing
                        st.write("**ðŸ” DEBUG: Batch Data After Processing**")
                        st.write(f"Total files in batch: {batch_data['total_files']}")
                        st.write(f"Total characters: {batch_data['total_characters']}")
                        st.write(f"Total chunks: {batch_data['total_chunks']}")
                        
                        # Process sitemap file (optional)
                        if sitemap_file:
                            if sitemap_file.type == "application/vnd.openxmlformats-officedocument.wordprocessingml.document":
                                sitemap_content = enhanced_processor.extract_text_from_docx(sitemap_file)
                            else:
                                sitemap_content = enhanced_processor.extract_text_from_txt(sitemap_file)
                        else:
                            sitemap_content = ""
                        
                        # Store in session state
                        st.session_state.batch_data = batch_data
                        st.session_state.sitemap_content = sitemap_content
                        # FIXED: Set scraped_content to the combined content from all files
                        st.session_state.scraped_content = batch_data['combined_content']
                        st.session_state.files_processed = True
                        
                        # Show results
                        if batch_data['total_characters'] > 0:
                            st.success(f"""
                            ðŸ“„ **Enhanced Processing Complete!**
                            - Files processed: {batch_data['total_files']}
                            - Total chunks created: {batch_data['total_chunks']}
                            - Total words: {batch_data['total_words']:,}
                            - Total characters: {batch_data['total_characters']:,}
                            """)
                        else:
                            st.error("âŒ **No content extracted from files!** Check the debug info above.")
                        
                        logger.info(f"Processed {len(scraped_corpus_files)} corpus files and {1 if sitemap_file else 0} sitemap file(s)")
                except Exception as e:
                    logger.error(f"File processing error: {e}")
                    st.error(f"File processing error: {str(e)}")
                    st.session_state.files_processed = False
            else:
                st.warning("Please upload at least one corpus file before processing.")
        
        st.markdown("---")
        st.info("**BEDROCK CLAUDE COMPATIBILITY:**")
        st.code("""
# FIXED: Using non-strict JSON schema
output_type=AgentOutputSchema(
    ObjectiveDataBedrock, 
    strict_json_schema=False  # Required for Bedrock Claude
)

# Required AWS Credentials
AWS_ACCESS_KEY_ID=AKIA...
AWS_SECRET_ACCESS_KEY=...
AWS_REGION=eu-north-1

# LiteLLM with Bedrock
pip install litellm

# EU Claude 3.7 Sonnet Profile
eu.anthropic.claude-3-7-sonnet-20250219-v1:0
        """)
    
    # Main content area
    if st.session_state.files_processed and st.session_state.batch_data:
        # Professional debug info for step-by-step inspection
        st.write("ðŸ” **DEBUG INFO:**")
        st.write(f"- Files uploaded: {len(scraped_corpus_files) if scraped_corpus_files else 0}")
        st.write(f"- Combined content length: {len(st.session_state.scraped_content):,} characters")
        st.write(f"- File markers in content: {st.session_state.scraped_content.count('=== FILE:')}")
        
        # Display file info
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("Files Processed", len(scraped_corpus_files) if scraped_corpus_files else 0)
        with col2:
            st.metric("Scraped Content Length", f"{len(st.session_state.scraped_content):,} characters")
        with col3:
            st.metric("Sitemap Content Length", f"{len(st.session_state.sitemap_content):,} characters")
        
        # Show a preview of processed files
        with st.expander("ðŸ“‹ Preview Processed Content"):
            st.text_area("Combined Scraped Content (first 1000 chars)", 
                        st.session_state.scraped_content[:1000] + "..." if len(st.session_state.scraped_content) > 1000 else st.session_state.scraped_content,
                        height=200)
        
        # ANALYSIS BUTTON - Bedrock Claude Compatible
        st.markdown("---")
        col1, col2 = st.columns([3, 1])
        
        with col1:
            if st.button("ðŸš€ Start Brand Analysis (BEDROCK CLAUDE FIX)", type="primary", use_container_width=True):
                
                # Initialize orchestrator
                orchestrator = BrandAnalysisOrchestrator()
                
                # Create progress tracking
                progress_bar = st.progress(0)
                status_text = st.empty()
                
                try:
                    # Run analysis with enhanced tracking
                    progress_bar.progress(10)
                    status_text.text("Initializing Bedrock Claude agents with non-strict JSON schema...")
                    
                    # Show debug info about what's being sent to the agent
                    with st.expander("ðŸ” DEBUG: Content Being Sent to First Agent", expanded=False):
                        st.write("**Content Length:**", len(st.session_state.scraped_content))
                        st.write("**File Count:**", st.session_state.scraped_content.count("FILE_NAME:"))
                        st.write("**Content Preview (first 2000 chars):**")
                        st.code(st.session_state.scraped_content[:2000] + "..." if len(st.session_state.scraped_content) > 2000 else st.session_state.scraped_content)
                    
                    # ENHANCED: Run the analysis pipeline with agent results tracking
                    objective_data, synthesis_report, agent_results = asyncio.run(
                        orchestrator.run_analysis(
                            st.session_state.scraped_content, 
                            st.session_state.sitemap_content
                        )
                    )
                    
                    progress_bar.progress(100)
                    status_text.text("Analysis completed with Bedrock Claude non-strict JSON schema!")
                    
                    # Store ENHANCED results in session state
                    st.session_state.analysis_results = {
                        'objective_data': objective_data,
                        'synthesis_report': synthesis_report,
                        'agent_results': agent_results  # ENHANCEMENT: Store detailed agent results
                    }
                    
                    # ENHANCEMENT: Show immediate success metrics
                    col1, col2, col3, col4 = st.columns(4)
                    
                    with col1:
                        status_icon = "âœ…" if agent_results['extraction_success'] else "âŒ"
                        st.metric("Extraction Agent", status_icon)
                    
                    with col2:
                        status_icon = "âœ…" if agent_results['synthesis_success'] else "âŒ"
                        st.metric("Synthesis Agent", status_icon)
                    
                    with col3:
                        st.metric("Pages Analyzed", objective_data.corpus_baseline.total_pages_analyzed)
                    
                    with col4:
                        st.metric("Words Analyzed", f"{objective_data.corpus_baseline.total_words_analyzed:,}")
                    
                    st.success("ðŸŽ‰ Brand analysis completed successfully with Bedrock Claude!")
                    
                    # ENHANCEMENT: Show any errors encountered
                    if agent_results['extraction_error'] or agent_results['synthesis_error']:
                        with st.expander("âš ï¸ Agent Execution Details", expanded=False):
                            if agent_results['extraction_error']:
                                st.error(f"**Extraction Error:** {agent_results['extraction_error']}")
                            if agent_results['synthesis_error']:
                                st.error(f"**Synthesis Error:** {agent_results['synthesis_error']}")
                
                except Exception as e:
                    logger.error(f"Analysis error: {e}")
                    st.error(f"Analysis error: {str(e)}")
                    progress_bar.progress(0)
                    status_text.text("Analysis failed.")
        
        with col2:
            if st.button("ðŸ”„ Reset Analysis", use_container_width=True):
                st.session_state.analysis_results = None
                st.success("Analysis results cleared!")
        
        # ENHANCED: Display results with agent details and new tabs
        if st.session_state.analysis_results:
            objective_data = st.session_state.analysis_results['objective_data']
            synthesis_report = st.session_state.analysis_results['synthesis_report']
            agent_results = st.session_state.analysis_results.get('agent_results', {})  # Handle backward compatibility
            
            # Create ENHANCED tabs for results
            tab1, tab2, tab3, tab4, tab5 = st.tabs([
                "ðŸ“Š Final Report", 
                "ðŸ” THE AUDITOR (First Agent)", 
                "ðŸ§¬ THE ARCHAEOLOGIST (Second Agent)", 
                "ðŸ“ˆ Analysis Metrics", 
                "ðŸ“¥ Downloads"
            ])
            
            with tab1:
                st.markdown("## Brand Discovery Report (Bedrock Claude)")
                st.markdown(synthesis_report.report_markdown)
            
            with tab2:  # NEW TAB: First Agent Results
                st.markdown("## ðŸ” THE AUDITOR - Extraction Agent Results")
                
                # Show execution status
                if agent_results.get('extraction_success'):
                    st.success("âœ… Extraction agent executed successfully")
                else:
                    st.error("âŒ Extraction agent encountered issues")
                    if agent_results.get('extraction_error'):
                        st.error(f"Error: {agent_results['extraction_error']}")
                
                # Display structured extraction results
                col1, col2 = st.columns(2)
                
                with col1:
                    st.subheader("ðŸ“Š Corpus Analysis Results")
                    st.metric("Total Pages Analyzed", objective_data.corpus_baseline.total_pages_analyzed)
                    st.metric("Total Words Analyzed", f"{objective_data.corpus_baseline.total_words_analyzed:,}")
                    st.metric("Pages Excluded", objective_data.corpus_baseline.total_pages_excluded)
                    
                    st.subheader("ðŸ“ Narrative Elements")
                    st.write(f"**Action Verbs Found:** {len(objective_data.narrative_data_points.action_verbs)}")
                    if objective_data.narrative_data_points.action_verbs:
                        st.write("*Top 15 Action Verbs:*")
                        for i, verb in enumerate(objective_data.narrative_data_points.action_verbs[:15], 1):
                            st.write(f"{i:2}. {verb}")
                    
                    st.write(f"**Future Statements Found:** {len(objective_data.narrative_data_points.future_statements)}")
                    if objective_data.narrative_data_points.future_statements:
                        st.write("*Sample Future Statements:*")
                        for i, stmt in enumerate(objective_data.narrative_data_points.future_statements[:5], 1):
                            st.write(f"{i}. {stmt}")
                
                with col2:
                    st.subheader("ðŸ¢ Business Structure Discovery")
                    
                    st.write(f"**Product Groups:** {len(objective_data.emergent_business_structures.discovered_product_groups)}")
                    if objective_data.emergent_business_structures.discovered_product_groups:
                        for group in objective_data.emergent_business_structures.discovered_product_groups:
                            st.write(f"â€¢ {group}")
                    
                    st.write(f"**Strategic Themes:** {len(objective_data.emergent_business_structures.discovered_strategic_themes)}")
                    if objective_data.emergent_business_structures.discovered_strategic_themes:
                        for theme in objective_data.emergent_business_structures.discovered_strategic_themes:
                            st.write(f"â€¢ {theme}")
                    
                    st.subheader("ðŸŽ¯ Audience Analysis")
                    st.write(f"**Audience Cues Found:** {len(objective_data.audience_data.audience_cues)}")
                    if objective_data.audience_data.audience_cues:
                        for cue in objective_data.audience_data.audience_cues[:10]:
                            st.write(f"â€¢ {cue}")
                
                # Show linguistic analysis
                st.subheader("ðŸ”¤ Verbal Identity Analysis")
                col3, col4 = st.columns(2)
                
                with col3:
                    st.write("**Lexical Frequency Analysis:**")
                    if hasattr(objective_data.verbal_identity_data_points, 'lexical_frequency'):
                        lex_freq = objective_data.verbal_identity_data_points.lexical_frequency
                        if hasattr(lex_freq, 'top_nouns') and lex_freq.top_nouns:
                            st.write("*Top Nouns:*")
                            for noun in lex_freq.top_nouns[:10]:
                                st.write(f"â€¢ {noun}")
                        
                        if hasattr(lex_freq, 'top_verbs') and lex_freq.top_verbs:
                            st.write("*Top Verbs:*")
                            for verb in lex_freq.top_verbs[:10]:
                                st.write(f"â€¢ {verb}")
                
                with col4:
                    st.write("**Nuanced Signal Detection:**")
                    st.write(f"*Value-laden Words:* {len(objective_data.nuanced_signals.value_laden_words)}")
                    for word in objective_data.nuanced_signals.value_laden_words[:10]:
                        st.write(f"â€¢ {word}")
                    
                    st.write(f"*Differentiation Markers:* {len(objective_data.nuanced_signals.differentiation_markers)}")
                    for marker in objective_data.nuanced_signals.differentiation_markers[:10]:
                        st.write(f"â€¢ {marker}")
                
                # Show raw agent output for debugging
                if agent_results.get('extraction_raw_output'):
                    with st.expander("ðŸ”§ Raw Agent Output (Debug)", expanded=False):
                        st.code(agent_results['extraction_raw_output'])
            
            with tab3:  # NEW TAB: Second Agent Results
                st.markdown("## ðŸ§¬ THE ARCHAEOLOGIST - Synthesis Agent Results")
                
                # Show execution status
                if agent_results.get('synthesis_success'):
                    st.success("âœ… Synthesis agent executed successfully")
                else:
                    st.error("âŒ Synthesis agent encountered issues")
                    if agent_results.get('synthesis_error'):
                        st.error(f"Error: {agent_results['synthesis_error']}")
                
                # Show synthesis metadata
                col1, col2 = st.columns(2)
                
                with col1:
                    st.subheader("ðŸ“‹ Synthesis Metadata")
                    st.write(f"**Analysis Date:** {synthesis_report.analysis_date}")
                    st.write(f"**Summary:** {synthesis_report.summary}")
                    st.write(f"**Report Length:** {len(synthesis_report.report_markdown):,} characters")
                    
                    # Count sections in the report
                    section_count = synthesis_report.report_markdown.count('##')
                    subsection_count = synthesis_report.report_markdown.count('###')
                    st.write(f"**Report Sections:** {section_count}")
                    st.write(f"**Report Subsections:** {subsection_count}")
                
                with col2:
                    st.subheader("ðŸ“Š Report Structure Analysis")
                    
                    # Analyze report content
                    report_lines = synthesis_report.report_markdown.split('\n')
                    non_empty_lines = [line for line in report_lines if line.strip()]
                    
                    st.write(f"**Total Lines:** {len(report_lines)}")
                    st.write(f"**Content Lines:** {len(non_empty_lines)}")
                    st.write(f"**Word Count:** {len(synthesis_report.report_markdown.split()):,}")
                    
                    # Check for key sections
                    has_executive_summary = "Executive Summary" in synthesis_report.report_markdown
                    has_mission = "Mission:" in synthesis_report.report_markdown
                    has_vision = "Vision:" in synthesis_report.report_markdown
                    has_values = "Values:" in synthesis_report.report_markdown
                    
                    st.write("**Key Sections Present:**")
                    st.write(f"â€¢ Executive Summary: {'âœ…' if has_executive_summary else 'âŒ'}")
                    st.write(f"â€¢ Mission: {'âœ…' if has_mission else 'âŒ'}")
                    st.write(f"â€¢ Vision: {'âœ…' if has_vision else 'âŒ'}")
                    st.write(f"â€¢ Values: {'âœ…' if has_values else 'âŒ'}")
                
                # Preview of report structure
                st.subheader("ðŸ“– Report Structure Preview")
                
                # Extract headers for structure overview
                headers = []
                for line in synthesis_report.report_markdown.split('\n'):
                    if line.startswith('#'):
                        level = len(line) - len(line.lstrip('#'))
                        header_text = line.lstrip('#').strip()
                        headers.append((level, header_text))
                
                if headers:
                    st.write("**Report Outline:**")
                    for level, header in headers[:20]:  # Show first 20 headers
                        indent = "  " * (level - 1)
                        st.write(f"{indent}{'#' * level} {header}")
                    
                    if len(headers) > 20:
                        st.write(f"... and {len(headers) - 20} more sections")
                
                # Show raw synthesis output for debugging
                if agent_results.get('synthesis_raw_output'):
                    with st.expander("ðŸ”§ Raw Agent Output (Debug)", expanded=False):
                        raw_output = agent_results['synthesis_raw_output']
                        st.code(raw_output[:3000] + "..." if len(raw_output) > 3000 else raw_output)
            
            # Keep tabs 4 and 5 as they were (Analysis Metrics and Downloads)
            with tab4:
                st.markdown("## Analysis Metrics & Metadata (Bedrock Claude)")
                
                # Display metadata from structured output
                col1, col2 = st.columns(2)
                
                with col1:
                    st.subheader("ðŸ“‹ Audit Information")
                    st.write(f"**Date:** {objective_data.audit_metadata.audit_date}")
                    st.write(f"**Agent:** {objective_data.audit_metadata.agent_name}")
                    st.write(f"**Persona:** {objective_data.audit_metadata.persona}")
                    
                    st.subheader("ðŸ“ˆ Linguistic Metrics")
                    if objective_data.verbal_identity_data_points.linguistic_metrics:
                        for key, value in objective_data.verbal_identity_data_points.linguistic_metrics.items():
                            st.write(f"**{key.replace('_', ' ').title()}:** {value}")
                
                with col2:
                    st.subheader("ðŸ“Š Synthesis Information")
                    st.write(f"**Analysis Date:** {synthesis_report.analysis_date}")
                    st.write(f"**Summary:** {synthesis_report.summary}")
                    
                    st.subheader("ðŸ”¢ Emotional Language Score")
                    if objective_data.nuanced_signals.emotional_language_score:
                        for emotion, score in objective_data.nuanced_signals.emotional_language_score.items():
                            st.write(f"**{emotion.title()}:** {score}")
                    
                    st.subheader("ðŸ”§ Technical Details")
                    st.write("**JSON Schema Mode:** Non-Strict (Bedrock Claude Compatible)")
                    st.write("**Model Provider:** LiteLLM + AWS Bedrock")
                    st.write("**Claude Version:** 3.7 Sonnet EU")
            
            with tab5:
                st.markdown("## Download Options (Bedrock Claude Compatible)")
                
                col1, col2 = st.columns(2)
                
                with col1:
                    # Download Markdown from structured output
                    st.download_button(
                        label="ðŸ“„ Download Report (Markdown)",
                        data=synthesis_report.report_markdown,
                        file_name=f"brand_analysis_bedrock_claude_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md",
                        mime="text/markdown"
                    )
                
                with col2:
                    # Download structured JSON data
                    st.download_button(
                        label="ðŸ“Š Download Structured Data (JSON)",
                        data=objective_data.model_dump_json(indent=2),
                        file_name=f"objective_data_bedrock_claude_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
                        mime="application/json"
                    )
                
                # PDF conversion
                
    else:
        # Instructions when no files are processed
        st.info("ðŸ‘† Please upload and process your files using the sidebar to begin analysis.")
        
        with st.expander("ðŸ“‹ How to use this BEDROCK CLAUDE FIXED system"):
            st.markdown("""
            ### What's Fixed in This Version:
            
            âœ… **Non-Strict JSON Schema**: Using `AgentOutputSchema(Model, strict_json_schema=False)`
            âœ… **Bedrock Claude Compatibility**: Simplified Pydantic models for LiteLLM + Bedrock
            âœ… **Proper Error Handling**: Better fallback mechanisms for Bedrock Claude
            âœ… **LiteLLM Integration**: Optimized for AWS Bedrock EU inference profiles
            âœ… **Type Safety**: Maintained structured outputs without strict schema validation
            
            ### Key Technical Changes:
            
            **Before (Strict Mode - Caused Error):**
            ```python
            extraction_agent = Agent(
                name="The Auditor",
                output_type=ObjectiveDataBedrock  # Strict mode by default
            )
            ```
            
            **After (Non-Strict Mode - Works with Bedrock Claude):**
            ```python
            extraction_agent = Agent(
                name="The Auditor",
                output_type=AgentOutputSchema(
                    ObjectiveDataBedrock, 
                    strict_json_schema=False  # FIXED for Bedrock Claude
                )
            )
            ```
            
            ### Why This Fix Works:
            
            1. **LiteLLM + Bedrock Claude Limitation**: Bedrock Claude through LiteLLM doesn't support OpenAI's strict JSON schema validation
            2. **Non-Strict Schema Mode**: Allows the model to generate JSON that approximates the schema without strict validation
            3. **Simplified Pydantic Models**: Removed complex nested structures that could cause schema validation issues
            4. **Better Error Recovery**: Improved fallback mechanisms when JSON parsing fails
            
            ### Step-by-Step Guide:
            
            1. **Upload Files**: Use the sidebar to upload your scraped corpus files (.md, .txt) and sitemap file (.docx, .txt)
            2. **Process Files**: Click "Process Uploaded Files" to combine and prepare your content
            3. **Run Analysis**: Click "Start Brand Analysis (BEDROCK CLAUDE FIX)" to execute the two-agent pipeline
            4. **Review Results**: Examine the generated brand report and structured data
            5. **Download**: Export your results in Markdown, JSON, or PDF format
            
            ### System Architecture:
            
            - **The Auditor**: Extraction agent with non-strict `ObjectiveDataBedrock` output
            - **The Archaeologist**: Synthesis agent with non-strict `SynthesisReport` output
            - **AWS Bedrock EU**: Claude 3.7 Sonnet through EU inference profile
            - **LiteLLM Integration**: Handles Bedrock API calls with proper authentication
            - **OpenAI Agents SDK**: Framework with non-strict JSON schema support
            """)
        
        # Show system status
        with st.expander("ðŸ”§ System Status (BEDROCK CLAUDE FIXED)"):
            col1, col2 = st.columns(2)
            
            with col1:
                st.markdown("**Environment Check:**")
                aws_key_id = os.getenv('AWS_ACCESS_KEY_ID', '')
                aws_secret = os.getenv('AWS_SECRET_ACCESS_KEY', '')
                aws_region = os.getenv('AWS_REGION', 'us-east-1')
                
                # Validate AWS Access Key ID format
                if aws_key_id:
                    if aws_key_id.startswith('AKIA') or aws_key_id.startswith('ASIA'):
                        key_status = f"âœ… ({aws_key_id[:4]}***)"
                    else:
                        key_status = "âŒ Invalid format"
                else:
                    key_status = "âŒ Missing"
                
                aws_secret_status = "âœ…" if aws_secret else "âŒ"
                
                # Check for conflicting profile variables
                aws_profile = os.getenv('AWS_PROFILE')
                aws_default_profile = os.getenv('AWS_DEFAULT_PROFILE')
                
                profile_status = "âŒ" if (aws_profile or aws_default_profile) else "âœ…"
                profile_info = f" (conflicts: AWS_PROFILE={aws_profile}, AWS_DEFAULT_PROFILE={aws_default_profile})" if (aws_profile or aws_default_profile) else ""
                
                st.markdown(f"""
                - AWS Access Key: {key_status}
                - AWS Secret Key: {aws_secret_status}
                - AWS Region: {aws_region}
                - No Profile Conflicts: {profile_status}{profile_info}
                - LiteLLM: âœ… Ready
                """)
            
            with col2:
                st.markdown("**Agent Status (BEDROCK CLAUDE FIXED):**")
                st.markdown(f"""
                - Extraction Agent: âœ… Ready (Non-Strict JSON Schema)
                - Synthesis Agent: âœ… Ready (Non-Strict JSON Schema)
                - Document Processor: âœ… Ready
                - PDF Generator: âœ… Ready
                - Pydantic Models: âœ… Simplified for Bedrock
                - Bedrock Claude: âœ… Compatible
                - JSON Schema Mode: âœ… Non-Strict (strict_json_schema=False)
                """)

# Additional utility functions remain the same...
def check_bedrock_connection() -> bool:
    """Check if Bedrock connection is working"""
    try:
        client = get_bedrock_client()
        # Test connection with a simple call
        test_response = call_bedrock_claude("Hello", max_tokens=10, temperature=0.1)
        return "Error" not in test_response
    except Exception as e:
        logger.error(f"Bedrock connection check failed: {e}")
        return False

def get_system_info() -> Dict[str, Any]:
    """Get system information for debugging"""
    return {
        "aws_region": os.getenv('AWS_REGION', 'eu-north-1'),
        "has_aws_credentials": bool(os.getenv('AWS_ACCESS_KEY_ID') and os.getenv('AWS_SECRET_ACCESS_KEY')),
        "python_version": f"{os.sys.version_info.major}.{os.sys.version_info.minor}.{os.sys.version_info.micro}",
        "streamlit_version": st.__version__,
        "timestamp": datetime.now().isoformat(),
        "strict_json_schema": False,  # FIXED: Now using non-strict mode
        "bedrock_claude_compatible": True,  # FIXED: Added compatibility indicator
        "litellm_integration": True
    }

# Error handling and recovery functions
class BrandAnalysisError(Exception):
    """Custom exception for brand analysis errors"""
    pass

def handle_analysis_error(error: Exception, context: str = "") -> str:
    """Handle and format analysis errors"""
    error_msg = f"Error in {context}: {str(error)}"
    logger.error(error_msg)
    return error_msg

class EnhancedBrandAnalysisOrchestrator:
    """Enhanced orchestrator that combines batch processing with your existing system"""
    
    def __init__(self):
        self.doc_processor = EnhancedDocumentProcessor()
        self.batch_processor = BatchProcessingOrchestrator()
        
    def should_use_batch_processing(self, batch_data: Dict[str, Any]) -> bool:
        """Determine if batch processing should be used based on content size"""
        
        # Use batch processing if:
        # 1. Total content > 50,000 characters (context window concerns)
        # 2. More than 3 files
        # 3. Any single file > 20,000 characters
        
        total_chars = batch_data.get('total_characters', 0)
        total_files = batch_data.get('total_files', 0)
        
        if total_chars > 50000:
            return True
        if total_files > 3:
            return True
            
        # Check individual file sizes
        for file_data in batch_data.get('files', []):
            if file_data.get('original_size', 0) > 20000:
                return True
                
        return False
    
    async def run_intelligent_analysis(self, batch_data: Dict[str, Any], 
                                     sitemap_content: str) -> Tuple[Any, SynthesisReport, dict]:
        """
        INTELLIGENT ROUTING: Choose between batch processing and traditional processing
        """
        
        # Decide processing method
        use_batch = self.should_use_batch_processing(batch_data)
        
        if use_batch:
            st.info("ðŸ”„ **INTELLIGENT ROUTING**: Using batch processing for large document set")
            st.info(f"""
            ðŸ“Š **Batch Processing Triggered:**
            - Total characters: {batch_data['total_characters']:,}
            - Total files: {batch_data['total_files']}
            - Total chunks: {batch_data['total_chunks']}
            - Processing method: Smart semantic chunking
            """)
            
            # Use batch processing
            return await self.batch_processor.run_batch_analysis(batch_data, sitemap_content)
        else:
            st.info("ðŸ”„ **INTELLIGENT ROUTING**: Using traditional processing for smaller document set")
            
            # Convert batch data back to traditional format for existing system
            combined_content = self._batch_to_traditional_format(batch_data)
            
            # Use your existing BrandAnalysisOrchestrator
            traditional_orchestrator = BrandAnalysisOrchestrator()
            return await traditional_orchestrator.run_analysis(combined_content, sitemap_content)
    
    def _batch_to_traditional_format(self, batch_data: Dict[str, Any]) -> str:
        """Convert batch data back to traditional format for smaller datasets"""
        
        combined_content = []
        combined_content.append(f"TOTAL_FILES_TO_PROCESS: {batch_data['total_files']}")
        combined_content.append(f"PROCESSING_TIMESTAMP: {batch_data['processing_timestamp']}")
        combined_content.append("\n" + "="*80 + "\n")
        
        for file_data in batch_data['files']:
            file_header = f"""
{'='*60}
FILE_NAME: {file_data['file_name']}
FILE_SIZE: {file_data['original_size']} characters
FILE_WORD_COUNT: {file_data['original_word_count']} words
{'='*60}
"""
            combined_content.append(file_header)
            
            # Reconstruct content from chunks
            file_content = ""
            for chunk in file_data['chunks']:
                file_content += chunk['content'] + "\n"
            
            combined_content.append(file_content)
            
            file_footer = f"""
{'='*60}
END_OF_FILE: {file_data['file_name']}
{'='*60}
"""
            combined_content.append(file_footer)
        
        return '\n'.join(combined_content)

# =================== ENHANCED STREAMLIT INTERFACE ===================

def enhanced_main():
    """Enhanced main function with intelligent processing"""
    
    st.set_page_config(
        page_title="Enhanced Brand Analysis System",
        page_icon="ðŸ§¬",
        layout="wide",
        initial_sidebar_state="expanded"
    )
    
    st.title("ðŸ§¬ Enhanced Brand Analysis System")
    st.markdown("### AI-Powered Brand Discovery with **Intelligent Batch Processing**")
    
    # Show enhanced features
    st.info("""
    ðŸš€ **NEW: Intelligent Processing System**
    - **Smart Chunking**: Semantic boundary-aware document splitting
    - **Batch Processing**: Handle unlimited document sizes
    - **Intelligent Routing**: Automatically chooses optimal processing method
    - **Context Preservation**: Overlapping chunks maintain meaning
    - **Parallel Processing**: Concurrent chunk analysis for speed
    """)
    
    # Validate environment
    if not validate_environment():
        st.stop()
    
    # Initialize session state
    if 'files_processed' not in st.session_state:
        st.session_state.files_processed = False
    if 'batch_data' not in st.session_state:
        st.session_state.batch_data = None
    if 'sitemap_content' not in st.session_state:
        st.session_state.sitemap_content = ""
    if 'analysis_results' not in st.session_state:
        st.session_state.analysis_results = None
    
    # Enhanced sidebar
    with st.sidebar:
        st.header("ðŸ“ Enhanced Document Upload")
        
        # Multiple file uploader
        scraped_corpus_files = st.file_uploader(
            "Upload Multiple Scraped Corpus Files (.md, .txt)",
            type=['md', 'txt'],
            help="Upload multiple files - system automatically handles large datasets with smart chunking",
            accept_multiple_files=True
        )
        
        sitemap_file = st.file_uploader(
            "Upload Sitemap (.docx, .txt)",
            type=['docx', 'txt'],
            help="Upload the sitemap document"
        )
        
        if st.button("ðŸ“‚ Process Uploaded Files", use_container_width=True):
            if scraped_corpus_files and sitemap_file:
                try:
                    with st.spinner("Processing files..."):
                        # âœ… FIXED: Create instance first
                        enhanced_processor = EnhancedDocumentProcessor()
                        
                        # ðŸ” DEBUG: Check files before processing
                        st.write("**ðŸ” DEBUG: File Analysis Before Processing**")
                        for i, file in enumerate(scraped_corpus_files):
                            file.seek(0)  # Reset pointer
                            raw_content = file.read()
                            file.seek(0)  # Reset again
                            
                            st.write(f"File {i+1}: {file.name}")
                            st.write(f"  - Raw size: {len(raw_content)} bytes")
                            st.write(f"  - File type: {file.type}")
                            
                            # Try to decode
                            try:
                                decoded_content = raw_content.decode('utf-8', errors='ignore')
                                st.write(f"  - Decoded size: {len(decoded_content)} characters")
                                st.write(f"  - First 100 chars: {decoded_content[:100]}")
                            except Exception as e:
                                st.error(f"  - Decode error: {e}")
                        
                        # Process multiple scraped corpus files with chunking
                        st.info(f"Processing {len(scraped_corpus_files)} corpus files...")
                        batch_data = enhanced_processor.process_multiple_files_with_chunking(scraped_corpus_files)
                        
                        # ðŸ” DEBUG: Check batch data after processing
                        st.write("**ðŸ” DEBUG: Batch Data After Processing**")
                        st.write(f"Total files in batch: {batch_data['total_files']}")
                        st.write(f"Total characters: {batch_data['total_characters']}")
                        st.write(f"Total chunks: {batch_data['total_chunks']}")
                        
                        # Process sitemap file
                        if sitemap_file.type == "application/vnd.openxmlformats-officedocument.wordprocessingml.document":
                            sitemap_content = enhanced_processor.extract_text_from_docx(sitemap_file)
                        else:
                            sitemap_content = enhanced_processor.extract_text_from_txt(sitemap_file)
                        
                        # Store in session state
                        st.session_state.batch_data = batch_data
                        st.session_state.sitemap_content = sitemap_content
                        st.session_state.files_processed = True
                        
                        # Show results
                        if batch_data['total_characters'] > 0:
                            st.success(f"""
                            ðŸ“„ **Enhanced Processing Complete!**
                            - Files processed: {batch_data['total_files']}
                            - Total chunks created: {batch_data['total_chunks']}
                            - Total words: {batch_data['total_words']:,}
                            - Total characters: {batch_data['total_characters']:,}
                            """)
                        else:
                            st.error("âŒ **No content extracted from files!** Check the debug info above.")
                        
                        logger.info(f"Processed {len(scraped_corpus_files)} corpus files and 1 sitemap file")
                except Exception as e:
                    logger.error(f"File processing error: {e}")
                    st.error(f"File processing error: {str(e)}")
                    st.session_state.files_processed = False
            else:
                st.warning("Please upload both corpus files and sitemap file before processing.")
        
        st.markdown("---")
        st.info("**ðŸ”§ Smart Processing Features:**")
        st.code("""
# Intelligent Processing Decision Tree
if total_size > 50K chars:
    â†’ Batch Processing
elif files > 3:
    â†’ Batch Processing  
elif any_file > 20K chars:
    â†’ Batch Processing
else:
    â†’ Traditional Processing

# Semantic Chunking Parameters
chunk_size = 8000 chars
overlap = 800 chars (10%)
boundary = paragraph/sentence
        """)
    
    # Main content area
    if st.session_state.files_processed and st.session_state.batch_data:
        batch_data = st.session_state.batch_data
        
        # Enhanced debug info
        st.write("ðŸ” **ENHANCED DEBUG INFO:**")
        st.write(f"- Files uploaded: {batch_data['total_files']}")
        st.write(f"- Total chunks created: {batch_data['total_chunks']}")
        st.write(f"- Total characters: {batch_data['total_characters']:,}")
        st.write(f"- Processing method: {batch_data['chunking_strategy']}")
        
        # Enhanced metrics display
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("Files Processed", batch_data['total_files'])
        with col2:
            st.metric("Smart Chunks Created", batch_data['total_chunks'])
        with col3:
            st.metric("Total Words", f"{batch_data['total_words']:,}")
        with col4:
            st.metric("Total Characters", f"{batch_data['total_characters']:,}")
        
        # Show enhanced preview of processed files
        with st.expander("ðŸ“‹ Enhanced Processing Preview"):
            st.write("**ðŸ“Š Processing Summary:**")
            for file_data in batch_data['files']:
                st.write(f"""
                **ðŸ“„ {file_data['file_name']}**
                - Original size: {file_data['original_size']:,} characters
                - Word count: {file_data['original_word_count']:,} words  
                - Chunks created: {file_data['chunk_count']}
                - Average chunk size: {file_data['original_size'] // file_data['chunk_count']:,} chars
                """)
        
        with col1:
            if st.button("ðŸš€ Start Enhanced Brand Analysis", type="primary", use_container_width=True):
                
                # Check if we should use batch processing
                orchestrator = EnhancedBrandAnalysisOrchestrator()
                use_batch_processing = orchestrator.should_use_batch_processing(batch_data)
                
                # Create progress tracking
                progress_bar = st.progress(0)
                status_text = st.empty()
                
                try:
                    progress_bar.progress(10)
                    
                    if use_batch_processing:
                        status_text.text("ðŸ”„ Using batch processing for large dataset...")
                        st.info(f"""
                        ðŸ“Š **BATCH PROCESSING MODE ACTIVATED**
                        - Dataset size: {batch_data['total_characters']:,} characters
                        - Files: {batch_data['total_files']}
                        - Chunks: {batch_data['total_chunks']}
                        - Method: Smart semantic chunking with overlap
                        """)
                        
                        # Use batch processing
                        objective_data, synthesis_report, agent_results = asyncio.run(
                            orchestrator.batch_processor.run_batch_analysis(
                                batch_data, 
                                st.session_state.sitemap_content
                            )
                        )
                    else:
                        status_text.text("âš¡ Using traditional processing for optimal dataset size...")
                        st.info("ðŸ“Š **TRADITIONAL PROCESSING MODE** - Dataset size is optimal for direct processing")
                        
                        # Convert batch data to traditional format
                        combined_content = orchestrator._batch_to_traditional_format(batch_data)
                        
                        # Use traditional processing
                        traditional_orchestrator = BrandAnalysisOrchestrator()
                        objective_data, synthesis_report, agent_results = asyncio.run(
                            traditional_orchestrator.run_analysis(
                                combined_content, 
                                st.session_state.sitemap_content
                            )
                        )
                    
                    progress_bar.progress(100)
                    status_text.text("âœ… Enhanced analysis completed!")
                    
                    # Store enhanced results
                    st.session_state.analysis_results = {
                        'objective_data': objective_data,
                        'synthesis_report': synthesis_report,
                        'agent_results': agent_results,
                        'processing_method': 'batch' if use_batch_processing else 'traditional',
                        'batch_data': batch_data
                    }
                    
                    # Show completion metrics
                    col1, col2, col3, col4 = st.columns(4)
                    
                    with col1:
                        method_icon = "ðŸ”„" if use_batch_processing else "âš¡"
                        st.metric("Processing Method", f"{method_icon} {'Batch' if use_batch_processing else 'Traditional'}")
                    
                    with col2:
                        if use_batch_processing:
                            st.metric("Chunks Processed", agent_results.get('total_chunks_processed', batch_data['total_chunks']))
                        else:
                            st.metric("Pages Analyzed", objective_data.corpus_baseline.total_pages_analyzed)
                    
                    with col3:
                        processing_time = agent_results.get('processing_time', 0)
                        st.metric("Processing Time", f"{processing_time:.1f}s")
                    
                    with col4:
                        words_analyzed = objective_data.corpus_baseline.total_words_analyzed
                        st.metric("Words Analyzed", f"{words_analyzed:,}")
                    
                    if use_batch_processing:
                        st.success(f"""
                        ðŸŽ‰ **Enhanced Batch Analysis Complete!**
                        - **Complete coverage**: All {batch_data['total_chunks']} chunks processed
                        - **No content loss**: Every piece of your {batch_data['total_characters']:,} characters analyzed
                        - **Smart aggregation**: Results combined using frequency-based ranking
                        - **Context preserved**: Overlapping chunks maintained semantic meaning
                        """)
                    else:
                        st.success("ðŸŽ‰ Traditional analysis completed successfully!")
                except Exception as e:
                    logger.error(f"Enhanced analysis error: {e}")
                    st.error(f"Analysis error: {str(e)}")
                    progress_bar.progress(0)
                    status_text.text("Analysis failed.")
        
        with col2:
            if st.button("ðŸ”„ Reset Analysis", use_container_width=True):
                st.session_state.analysis_results = None
                st.success("Analysis results cleared!")
        
        # Enhanced results display
        if st.session_state.analysis_results:
            results = st.session_state.analysis_results
            objective_data = results['objective_data']
            synthesis_report = results['synthesis_report']
            agent_results = results['agent_results']
            processing_method = results['processing_method']
            
            # Enhanced tabs with processing method info
            if processing_method == 'batch':
                tab1, tab2, tab3, tab4, tab5, tab6 = st.tabs([
                    "ðŸ“Š Final Report", 
                    "ðŸ”„ Batch Processing Details",
                    "ðŸ” THE AUDITOR (Extraction)", 
                    "ðŸ§¬ THE ARCHAEOLOGIST (Synthesis)", 
                    "ðŸ“ˆ Analysis Metrics", 
                    "ðŸ“¥ Downloads"
                ])
                
                with tab2:  # New tab for batch processing details
                    st.markdown("## ðŸ”„ Batch Processing Analysis Details")
                    
                    if hasattr(objective_data, 'batch_metadata'):
                        batch_meta = objective_data.batch_metadata
                        
                        col1, col2 = st.columns(2)
                        
                        with col1:
                            st.subheader("ðŸ“Š Batch Processing Statistics")
                            st.metric("Batch ID", batch_meta.batch_id)
                            st.metric("Total Chunks Processed", batch_meta.total_chunks)
                            st.metric("Total Files Analyzed", batch_meta.total_files)
                            st.metric("Processing Duration", f"{agent_results.get('processing_time', 0):.1f}s")
                            
                            st.subheader("âš¡ Processing Efficiency")
                            chunks_per_second = batch_meta.total_chunks / max(agent_results.get('processing_time', 1), 1)
                            st.metric("Chunks/Second", f"{chunks_per_second:.2f}")
                            
                            words_per_second = objective_data.corpus_baseline.total_words_analyzed / max(agent_results.get('processing_time', 1), 1)
                            st.metric("Words/Second", f"{words_per_second:,.0f}")
                        
                        with col2:
                            st.subheader("ðŸŽ¯ Content Analysis Results")
                            
                            if hasattr(objective_data, 'chunk_results'):
                                chunk_results = objective_data.chunk_results
                                
                                st.write(f"**Individual Chunk Results:** {len(chunk_results)} chunks")
                                
                                # Analyze chunk distribution
                                files_processed = {}
                                for chunk in chunk_results:
                                    file_name = chunk.file_name
                                    if file_name not in files_processed:
                                        files_processed[file_name] = 0
                                    files_processed[file_name] += 1
                                
                                st.write("**Chunks per File:**")
                                for file_name, chunk_count in files_processed.items():
                                    st.write(f"â€¢ {file_name}: {chunk_count} chunks")
                            
                            st.subheader("ðŸ“ˆ Aggregation Summary")
                            st.write(f"â€¢ Action verbs aggregated: {len(objective_data.narrative_data_points.action_verbs)}")
                            st.write(f"â€¢ Product groups found: {len(objective_data.emergent_business_structures.discovered_product_groups)}")
                            st.write(f"â€¢ Strategic themes: {len(objective_data.emergent_business_structures.discovered_strategic_themes)}")
                        
                        # Show chunk-level insights
                        if hasattr(objective_data, 'chunk_results') and objective_data.chunk_results:
                            st.subheader("ðŸ” Chunk-Level Analysis Insights")
                            
                            chunk_data = []
                            for chunk in objective_data.chunk_results[:10]:  # Show first 10 chunks
                                chunk_data.append({
                                    'File': chunk.file_name,
                                    'Chunk': chunk.chunk_index + 1,
                                    'Words': chunk.word_count,
                                    'Action Verbs': len(chunk.action_verbs),
                                    'Future Statements': len(chunk.future_statements),
                                    'Product Groups': len(chunk.product_groups),
                                    'Value Words': len(chunk.value_words)
                                })
                            
                            if chunk_data:
                                df = pd.DataFrame(chunk_data)
                                st.dataframe(df, use_container_width=True)
                                
                                if len(objective_data.chunk_results) > 10:
                                    st.info(f"Showing first 10 chunks. Total chunks processed: {len(objective_data.chunk_results)}")
            
            else:
                # Traditional tabs for non-batch processing
                tab1, tab2, tab3, tab4, tab5 = st.tabs([
                    "ðŸ“Š Final Report", 
                    "ðŸ” THE AUDITOR (First Agent)", 
                    "ðŸ§¬ THE ARCHAEOLOGIST (Second Agent)", 
                    "ðŸ“ˆ Analysis Metrics", 
                    "ðŸ“¥ Downloads"
                ])
            
            # Rest of tabs remain the same as your existing implementation...
            with tab1:
                st.markdown("## Brand Discovery Report")
                st.markdown(synthesis_report.report_markdown)
            
            # ... (keep other tabs as in your existing code)
    
    else:
        # Instructions when no files are processed
        st.info("ðŸ‘† Please upload and process your files using the enhanced sidebar to begin analysis.")
        
        with st.expander("ðŸ“‹ Enhanced System Capabilities"):
            st.markdown("""
            ### ðŸš€ Intelligent Processing System
            
            Our enhanced system automatically determines the best processing method for your documents:
            
            #### ðŸ”„ **Batch Processing Mode** (Large Datasets)
            - **Triggers when**: >50K characters, >3 files, or any file >20K characters
            - **Smart Chunking**: Semantic boundary-aware splitting
            - **Context Preservation**: 10% overlap between chunks
            - **Parallel Processing**: Multiple chunks analyzed concurrently
            - **Complete Coverage**: Every section analyzed individually
            
            #### âš¡ **Traditional Processing Mode** (Smaller Datasets)
            - **Triggers when**: Smaller, manageable datasets
            - **Direct Processing**: All content processed together
            - **Faster Results**: Optimized for smaller documents
            - **Context Integrity**: Maintains full document context
            
            #### ðŸ§  **Smart Features**
            - **Automatic Detection**: System chooses optimal method
            - **Semantic Chunking**: Preserves meaning at boundaries
            - **Overlap Strategy**: Maintains context between chunks
            - **Aggregated Results**: Intelligent combining of findings
            - **Performance Optimized**: Concurrent processing where beneficial
            
            #### ðŸ“Š **Best Practices Applied**
            - Based on research from Pinecone, MongoDB, Chroma
            - 8,000 character optimal chunk size (â‰ˆ2,000 tokens)
            - Semantic boundary preservation
            - Frequency-based aggregation
            - Context window optimization
            """)

if __name__ == "__main__":
    try:
        # Check for required dependencies
        try:
            import litellm
            from pydantic import BaseModel
            from agents import AgentOutputSchema  # FIXED: Import AgentOutputSchema
            logger.info("LiteLLM, Pydantic, and AgentOutputSchema integration available")
        except ImportError as e:
            st.error(f"Missing dependency: {e}. Please install with: pip install litellm pydantic 'openai-agents[litellm]'")
            st.stop()
        
        # Check for AWS Access Key format
        aws_key_id = os.getenv('AWS_ACCESS_KEY_ID', '')
        if aws_key_id and not (aws_key_id.startswith('AKIA') or aws_key_id.startswith('ASIA')):
            st.error("ðŸš¨ Invalid AWS Access Key ID Format!")
            st.markdown("""
            **Your AWS Access Key ID must start with:**
            - `AKIA` (for long-term IAM user credentials)
            - `ASIA` (for temporary STS credentials)
            
            **Current format is invalid.** Please create new access keys:
            
            1. Go to AWS Console â†’ IAM â†’ Users â†’ [Your User] â†’ Security Credentials
            2. Click "Create access key"
            3. Use the new key that starts with `AKIA`
            """)
            st.stop()
        
        # Check for AWS profile conflicts
        profile_vars = ['AWS_PROFILE', 'AWS_DEFAULT_PROFILE']
        conflicting_vars = [(var, os.getenv(var)) for var in profile_vars if os.getenv(var)]
        
        if conflicting_vars:
            st.error("âš ï¸ AWS Profile Conflict Detected!")
            st.write("The following environment variables are causing conflicts:")
            for var, value in conflicting_vars:
                st.code(f"{var}={value}")
            
            st.info("**How to fix:**")
            st.code("""
# In your terminal, run:
unset AWS_PROFILE
unset AWS_DEFAULT_PROFILE

# Then restart Streamlit
streamlit run your_app.py
            """)
            st.warning("Please clear these variables and restart the application.")
            st.stop()
        
        main()
    except Exception as e:
        logger.error(f"Application startup error: {e}")
        st.error(f"Application startup error: {str(e)}")
        st.info("Please check your configuration and try again.")